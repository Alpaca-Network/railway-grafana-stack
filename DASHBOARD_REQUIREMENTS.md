# ğŸ“Š Dashboard Requirements Analysis
**Created:** 2025-12-28
**Purpose:** Detailed specification of data sources, endpoints, and requirements for 6-dashboard monitoring suite
**Status:** ğŸ” Requirements Definition Phase

---

## Executive Summary

The 6-dashboard suite requires data from **7 major categories**. This document identifies every data point needed, its source, granularity, and expected format.

---

## ğŸ“ˆ Data Categories & Requirements

### Category 1: Provider/Gateway Metrics

**Definition**: Real-time and historical metrics for all 17 providers

**Required Metrics Per Provider**:
```
â”œâ”€â”€ Real-time Metrics
â”‚   â”œâ”€â”€ Health Score (0-100)
â”‚   â”œâ”€â”€ Status (healthy/warning/critical)
â”‚   â”œâ”€â”€ Requests/minute (current)
â”‚   â”œâ”€â”€ Errors/minute (current)
â”‚   â”œâ”€â”€ Error Rate % (current)
â”‚   â”œâ”€â”€ Avg Latency (p50, p95, p99, p999)
â”‚   â””â”€â”€ Uptime % (24h)
â”‚
â”œâ”€â”€ Financial Metrics
â”‚   â”œâ”€â”€ Cost (24h, 7d, 30d)
â”‚   â”œâ”€â”€ Cost per request
â”‚   â””â”€â”€ Cost trend
â”‚
â”œâ”€â”€ Token Metrics
â”‚   â”œâ”€â”€ Total tokens (24h, 7d, 30d)
â”‚   â”œâ”€â”€ Tokens/second (throughput)
â”‚   â””â”€â”€ Cost per million tokens
â”‚
â””â”€â”€ Availability Metrics
    â”œâ”€â”€ Availability % (24h, 7d, 30d)
    â””â”€â”€ Circuit breaker status
```

**Data Granularity Needed**:
- Real-time: 1-minute intervals (last 24h)
- Historical: 1-hour aggregates (7d, 30d)
- Latest value: Current health score, status

**Used By Dashboards**: All 6 dashboards
**Sample Endpoint Pattern**: `/api/providers/{provider_slug}/metrics`

---

### Category 2: Model-Level Metrics

**Definition**: Metrics broken down by specific model (gpt-4o, claude-sonnet, etc.)

**Required Metrics Per Model**:
```
â”œâ”€â”€ Usage Metrics
â”‚   â”œâ”€â”€ Total requests (24h, 7d, 30d)
â”‚   â”œâ”€â”€ Requests/second (throughput)
â”‚   â”œâ”€â”€ Error count & error rate %
â”‚   â””â”€â”€ Success rate %
â”‚
â”œâ”€â”€ Performance Metrics
â”‚   â”œâ”€â”€ Latency (p50, p95, p99, p999)
â”‚   â”œâ”€â”€ Response time distribution
â”‚   â””â”€â”€ Timeout rate %
â”‚
â”œâ”€â”€ Token Metrics
â”‚   â”œâ”€â”€ Input tokens (total & avg per request)
â”‚   â”œâ”€â”€ Output tokens (total & avg per request)
â”‚   â”œâ”€â”€ Completion ratio (output/input)
â”‚   â””â”€â”€ Cost per token
â”‚
â”œâ”€â”€ Financial Metrics
â”‚   â”œâ”€â”€ Cost (24h, 7d, 30d)
â”‚   â”œâ”€â”€ Cost per request
â”‚   â”œâ”€â”€ Cost vs budget (if model has budget)
â”‚   â””â”€â”€ Cost trend
â”‚
â””â”€â”€ Model Properties
    â”œâ”€â”€ Provider name (which provider hosts this model)
    â”œâ”€â”€ Model family (GPT, Claude, Llama, etc.)
    â””â”€â”€ Max tokens / context window
```

**Data Granularity Needed**:
- Real-time: 1-minute intervals (last 24h)
- Historical: 1-hour aggregates (7d)
- Latest value: Current requests/sec, cost/token

**Used By Dashboards**: Dashboard 2 (primary), Dashboard 4 (cost), Dashboard 6 (tokens)
**Sample Endpoint Pattern**: `/api/models/{model_id}/metrics` or `/api/models/list`

---

### Category 3: Error & Incident Data

**Definition**: Detailed error tracking for incident response

**Required Data Per Error**:
```
â”œâ”€â”€ Error Identification
â”‚   â”œâ”€â”€ Timestamp (when occurred)
â”‚   â”œâ”€â”€ Error type (timeout, rate_limit, invalid_request, server_error, auth_error)
â”‚   â”œâ”€â”€ Error message
â”‚   â””â”€â”€ Error code (if applicable)
â”‚
â”œâ”€â”€ Context
â”‚   â”œâ”€â”€ Affected provider
â”‚   â”œâ”€â”€ Affected model
â”‚   â”œâ”€â”€ Request ID (for tracing)
â”‚   â””â”€â”€ User/API key (for analysis)
â”‚
â”œâ”€â”€ Impact
â”‚   â”œâ”€â”€ Error count (recent)
â”‚   â”œâ”€â”€ Error rate % (recent)
â”‚   â””â”€â”€ Error trend (increasing/stable/decreasing)
â”‚
â””â”€â”€ Details (for drill-down)
    â”œâ”€â”€ Stack trace
    â”œâ”€â”€ Request payload (sanitized)
    â””â”€â”€ Response details
```

**Aggregate Metrics Needed**:
- Error rate % by provider (last 24h, 7d)
- Error rate % by model (last 24h)
- Error type distribution (pie chart data)
- Error count time series (5-min intervals)
- Recent error feed (tail of last 100 errors)

**Data Granularity Needed**:
- Real-time: 1-minute intervals
- Tail log: Last 100-1000 events with 5s refresh
- Historical: 1-hour aggregates for trends

**Used By Dashboards**: Dashboard 5 (primary), Dashboard 1 (alert list), Dashboard 3 (error rate comparison)
**Sample Endpoint Pattern**: `/api/errors` (tail), `/api/errors/stats/{provider}`, `/api/errors/stats/by-type`

---

### Category 4: Financial & Business Metrics

**Definition**: Revenue, cost, and budget tracking

**Required Data**:
```
â”œâ”€â”€ Revenue Metrics
â”‚   â”œâ”€â”€ Daily revenue (current day)
â”‚   â”œâ”€â”€ Revenue trend (7d, 30d)
â”‚   â”œâ”€â”€ Revenue by model/provider
â”‚   â””â”€â”€ Revenue vs cost (profit)
â”‚
â”œâ”€â”€ Cost Metrics
â”‚   â”œâ”€â”€ Daily cost breakdown
â”‚   â”‚   â”œâ”€â”€ By provider
â”‚   â”‚   â”œâ”€â”€ By model
â”‚   â”‚   â””â”€â”€ By customer/team
â”‚   â”œâ”€â”€ Cost trends (7d, 30d)
â”‚   â”œâ”€â”€ Cost per request (weighted avg)
â”‚   â””â”€â”€ Cost per token (weighted avg)
â”‚
â”œâ”€â”€ Budget & Allocation
â”‚   â”œâ”€â”€ Monthly budget (total)
â”‚   â”œâ”€â”€ Budget allocation by model
â”‚   â”œâ”€â”€ Budget allocation by provider
â”‚   â”œâ”€â”€ Current spend vs budget
â”‚   â”œâ”€â”€ Projected vs budget (forecast)
â”‚   â””â”€â”€ Budget alerts (% of budget used)
â”‚
â””â”€â”€ Financial Health
    â”œâ”€â”€ Profit margin % (revenue - cost / revenue)
    â”œâ”€â”€ Cost per request trend
    â”œâ”€â”€ Cost per token trend
    â””â”€â”€ Unit economics by model
```

**Data Granularity Needed**:
- Daily: Cost and revenue snapshots
- Hourly: Cost trends (for visualization)
- Real-time: Current day cost/revenue running totals
- Monthly/Historical: For trend lines

**Used By Dashboards**: Dashboard 4 (primary), Dashboard 1 (KPIs), Dashboard 2 (cost ranking)
**Sample Endpoint Pattern**: `/api/financial/revenue`, `/api/financial/costs`, `/api/financial/budget`

---

### Category 5: SLO & Availability Metrics

**Definition**: Service level objectives and availability tracking

**Required Data**:
```
â”œâ”€â”€ SLO Definition
â”‚   â”œâ”€â”€ Target availability % (e.g., 99.9%)
â”‚   â”œâ”€â”€ Target latency p95 (e.g., 500ms)
â”‚   â”œâ”€â”€ Target error rate % (e.g., <1%)
â”‚   â””â”€â”€ Reporting period (daily, weekly, monthly)
â”‚
â”œâ”€â”€ SLO Compliance
â”‚   â”œâ”€â”€ Current availability % vs target
â”‚   â”œâ”€â”€ Current latency vs target
â”‚   â”œâ”€â”€ Current error rate vs target
â”‚   â”œâ”€â”€ Compliance status (pass/fail/warning)
â”‚   â””â”€â”€ Compliance trend (7d, 30d)
â”‚
â”œâ”€â”€ Availability Metrics
â”‚   â”œâ”€â”€ Uptime % (provider level)
â”‚   â”œâ”€â”€ Downtime incidents (count, duration)
â”‚   â”œâ”€â”€ Incident timestamps
â”‚   â””â”€â”€ Incident severity
â”‚
â””â”€â”€ Circuit Breaker Status
    â”œâ”€â”€ Is breaker OPEN/CLOSED (per provider)
    â”œâ”€â”€ Time in current state
    â”œâ”€â”€ Failure count (why it opened)
    â””â”€â”€ Last failure timestamp
```

**Data Granularity Needed**:
- Real-time: Current SLO compliance
- Hourly: Availability %, uptime %
- Incidents: Timestamp + duration
- Circuit breaker: Current state (no history needed)

**Used By Dashboards**: Dashboard 5 (primary), Dashboard 1 (alerts), Dashboard 3 (uptime comparison)
**Sample Endpoint Pattern**: `/api/slo/status`, `/api/health/availability`, `/api/providers/{provider}/breaker-status`

---

### Category 6: Token Metrics (Detailed)

**Definition**: Comprehensive token usage and efficiency tracking

**Required Data**:
```
â”œâ”€â”€ Token Counts (by time period)
â”‚   â”œâ”€â”€ Total input tokens
â”‚   â”œâ”€â”€ Total output tokens
â”‚   â”œâ”€â”€ Total tokens (input + output)
â”‚   â”œâ”€â”€ Average tokens per request
â”‚   â””â”€â”€ Tokens per second (throughput)
â”‚
â”œâ”€â”€ Token Distribution
â”‚   â”œâ”€â”€ Tokens by model (top 10)
â”‚   â”œâ”€â”€ Tokens by provider
â”‚   â”œâ”€â”€ Input vs output ratio (per model)
â”‚   â””â”€â”€ Token cost breakdown
â”‚
â”œâ”€â”€ Token Efficiency
â”‚   â”œâ”€â”€ Estimated tokens vs actual
â”‚   â”œâ”€â”€ Efficiency ratio (how close to estimate)
â”‚   â”œâ”€â”€ Token waste % (over-estimation)
â”‚   â””â”€â”€ Token optimization opportunities
â”‚
â””â”€â”€ Cost Per Token
    â”œâ”€â”€ Weighted average $/token
    â”œâ”€â”€ $/token by provider
    â”œâ”€â”€ $/token by model
    â””â”€â”€ $/token trend (7d, 30d)
```

**Data Granularity Needed**:
- Hourly: Token counts for trends
- Per request: Input/output breakdown (for averaging)
- Daily: Cost per token calculations
- Historical: 7d, 30d aggregates

**Used By Dashboards**: Dashboard 6 (primary), Dashboard 2 (model comparison), Dashboard 4 (cost per token)
**Sample Endpoint Pattern**: `/api/tokens/usage`, `/api/tokens/stats/{model}`, `/api/tokens/efficiency`

---

### Category 7: Metadata & Configuration

**Definition**: Static/slowly-changing reference data

**Required Data**:
```
â”œâ”€â”€ Provider Definitions (17 total)
â”‚   â”œâ”€â”€ Provider name & slug
â”‚   â”œâ”€â”€ Provider status (active/deprecated/beta)
â”‚   â”œâ”€â”€ Assigned color (for consistent visualization)
â”‚   â”œâ”€â”€ Supported models (list)
â”‚   â”œâ”€â”€ Website/docs URL
â”‚   â””â”€â”€ Contact info
â”‚
â”œâ”€â”€ Model Definitions
â”‚   â”œâ”€â”€ Model name & ID
â”‚   â”œâ”€â”€ Model family (GPT/Claude/Llama/etc)
â”‚   â”œâ”€â”€ Provider (which provider hosts it)
â”‚   â”œâ”€â”€ Context window size
â”‚   â”œâ”€â”€ Max tokens (output limit)
â”‚   â”œâ”€â”€ Cost per 1M tokens (input/output separate)
â”‚   â”œâ”€â”€ Is deprecated? (boolean)
â”‚   â”œâ”€â”€ Release date
â”‚   â””â”€â”€ Description
â”‚
â”œâ”€â”€ Dashboard Configuration
â”‚   â”œâ”€â”€ Budget limits (monthly, by model)
â”‚   â”œâ”€â”€ SLO targets (latency, availability, error rate)
â”‚   â”œâ”€â”€ Alert thresholds (cost spike, error spike, etc.)
â”‚   â”œâ”€â”€ Refresh rates per dashboard
â”‚   â””â”€â”€ Color scheme/theme
â”‚
â””â”€â”€ User/Team Information
    â”œâ”€â”€ Team name
    â”œâ”€â”€ API keys (masked)
    â”œâ”€â”€ Assigned budget
    â””â”€â”€ Notification preferences
```

**Data Type**: Static/Reference - minimal refresh needed
**Used By**: All dashboards for labeling, filtering, drilling down
**Sample Endpoint Pattern**: `/api/providers/list`, `/api/models/list`, `/api/config/dashboard`

---

## ğŸ”„ Dashboard-to-Data Mapping

### Dashboard 1: Executive Overview
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Overall Health Score | Provider avg health (all 17) | 30s | Aggregate metric |
| Active Requests/min | Sum of all providers | 15s | Real-time |
| Avg Response Time | Weighted avg of all providers | 30s | By request volume |
| Daily Cost | Sum from financial system | 60s | Running total |
| Provider Health Grid (17) | Individual provider health | 60s | Color-coded status |
| Request Volume (24h) | Hourly aggregates | 30s | All providers combined |
| Error Rate Distribution | Error % by provider | 60s | Latest 24h data |
| Alert List | Errors/anomalies (recent) | 30s | Tail of incidents |

---

### Dashboard 2: Model Performance Analytics
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Top 5 Models | Models sorted by requests (7d) | 60s | From model metrics |
| Models with Issues | Models with high error % | 30s | >5% error threshold |
| Request Volume (7d) | Hourly by model | 60s | Stacked area/bar |
| Cost per Model | Cost totals ranked | 300s | 7d aggregates |
| Latency Distribution | P50, P95, P99 per model | 60s | Top 10 models |
| Success Rate vs Usage | XY: success% vs request count | 60s | Bubble size = cost |
| Performance Heatmap | Model x Time heatmap | 60s | Hour x model grid |
| Model Health Gauge | Weighted score (top 3) | 30s | Custom calculation |

---

### Dashboard 3: Gateway & Provider Comparison
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Health Gauge Grid (17) | Individual health scores | 60s | 6-column grid |
| Comparison Matrix | All metrics per provider | 300s | 10+ columns |
| Cost vs Reliability | XY: cost/req vs success% | 300s | Bubble = volume |
| Request Distribution | % by provider (pie) | 60s | 7d data |
| Cost Distribution | % by provider (pie) | 60s | 7d data |
| Latency Distribution | P95 by provider (violin) | 60s | Statistical view |
| Cost Trend | Line per provider (7d) | 300s | 1-hour granularity |
| Uptime Trend | Uptime % per provider (7d) | 300s | 1-hour granularity |

---

### Dashboard 4: Business & Financial Metrics
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Daily Revenue | Today's revenue total | 60s | Running total |
| Daily Cost | Today's cost total | 60s | Running total |
| Profit Margin | (Revenue - Cost) / Revenue % | 60s | Calculated |
| Cost by Model | Treemap hierarchical view | 300s | All models |
| Cost Trend | Area chart with budget line | 300s | 7d with rolling avg |
| Cost/Token vs Throughput | XY scatter | 300s | Efficiency view |
| Top 5 Expensive Models | Bar chart ranked | 300s | 7d totals |
| Cost Optimization Tips | Text recommendations | Static/600s | Auto-generated? |

---

### Dashboard 5: Real-Time Incident Response
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Alert List (top) | Critical anomalies | 15s | Sorted by severity |
| Error Rate (real-time) | Error % with threshold bands | 10s | 5-min granularity |
| SLO Compliance Gauge | % vs target (e.g., 99.9%) | 30s | Green/yellow/red |
| Recent Errors Table | Error log tail | 5s | Last 100 events |
| Circuit Breaker Status | OPEN/CLOSED per provider | 30s | 17-item grid |
| Availability Heatmap | 24h x 17 providers | 60s | Uptime visualization |
| Request Success Rate | % successful vs failed | 15s | Real-time trend |
| Application Logs | Raw log tail | 5s | Last 50 lines |

---

### Dashboard 6: Tokens & Throughput Analysis
| Panel | Required Data | Refresh | Notes |
|-------|--------------|---------|-------|
| Total Tokens (24h) | Sum of all tokens | 60s | Big stat |
| Tokens/Second | Current throughput | 30s | Real-time |
| Cost per 1M Tokens | Weighted average | 300s | Financial calc |
| Tokens by Model | Horizontal bar (input/output) | 60s | Top 10 models |
| Input:Output Ratio | XY scatter per model | 60s | Bubble = cost |
| Efficiency Gauge | Estimated vs actual ratio | 60s | 0-100 scale |
| Tokens/Sec Trend (7d) | Line stacked by provider | 300s | 1-hour granularity |
| Cost/Token Trend (7d) | Line with benchmark | 300s | 1-hour granularity |

---

## ğŸŒ API Endpoint Design Recommendations

### Provider Metrics Endpoints

**Option A: Comprehensive Single Endpoint**
```bash
GET /api/providers/metrics
Response: {
  timestamp: "2025-12-28T10:30:00Z",
  providers: [
    {
      slug: "openrouter",
      name: "OpenRouter",
      health_score: 95,
      status: "healthy",
      realtime: {
        requests_per_minute: 1245,
        errors_per_minute: 8,
        error_rate_percent: 0.64,
        avg_latency_ms: 245,
        p50_latency_ms: 120,
        p95_latency_ms: 450,
        p99_latency_ms: 890
      },
      availability: {
        uptime_24h_percent: 99.95,
        uptime_7d_percent: 99.87,
        circuit_breaker_open: false
      },
      financial: {
        cost_24h: 3456.78,
        cost_7d: 24123.45,
        cost_30d: 89234.56,
        cost_per_request: 0.00012
      },
      tokens: {
        total_24h: 1234567,
        tokens_per_second: 14285,
        cost_per_million: 2.80
      }
    },
    // ... 16 more providers
  ]
}
```

**Option B: Separate Endpoints (more granular)**
```bash
GET /api/providers/list
GET /api/providers/{slug}/health
GET /api/providers/{slug}/realtime
GET /api/providers/{slug}/availability
GET /api/providers/{slug}/financial
GET /api/providers/{slug}/tokens
```

### Model Metrics Endpoints

```bash
GET /api/models/list
Response: [
  {
    id: "gpt-4o",
    name: "GPT-4o",
    provider: "openrouter",
    model_family: "gpt",
    context_window: 128000,
    cost_per_1m_input: 5.00,
    cost_per_1m_output: 15.00
  },
  // ... all models
]

GET /api/models/{model_id}/metrics
Response: {
  model_id: "gpt-4o",
  timestamp: "2025-12-28T10:30:00Z",
  period: "24h",  // or "7d", "30d"
  usage: {
    total_requests: 45678,
    success_count: 45234,
    error_count: 444,
    success_rate_percent: 98.97,
    requests_per_second: 0.528
  },
  performance: {
    p50_latency_ms: 120,
    p95_latency_ms: 450,
    p99_latency_ms: 890,
    timeout_rate_percent: 0.12
  },
  tokens: {
    input_tokens: 234567890,
    output_tokens: 123456789,
    total_tokens: 358024679,
    avg_input_per_request: 5143,
    avg_output_per_request: 2705,
    tokens_per_second: 4141
  },
  financial: {
    total_cost: 1234.56,
    cost_per_request: 0.027,
    cost_per_token: 0.00000344
  }
}

GET /api/models/metrics/compare
Query params: ?model_ids=gpt-4o,claude-sonnet,llama-70b&period=7d
Response: [...] // Multiple models with comparable structure
```

### Error & Incident Endpoints

```bash
GET /api/errors/recent
Query params: ?limit=100&sort=newest
Response: [
  {
    timestamp: "2025-12-28T10:25:43Z",
    error_type: "timeout",
    error_message: "Request exceeded 30s timeout",
    provider: "together",
    model: "llama-2-70b",
    error_code: 504,
    request_id: "req_abc123..."
  },
  // ... 99 more
]

GET /api/errors/stats
Query params: ?period=24h
Response: {
  total_errors_24h: 1234,
  error_rate_percent: 0.92,
  errors_by_type: {
    timeout: 456,
    rate_limit: 234,
    auth_error: 88,
    invalid_request: 45,
    server_error: 411
  },
  errors_by_provider: {
    together: 345,
    openrouter: 123,
    // ... rest of providers
  },
  trending: "decreasing"  // or "stable", "increasing"
}

GET /api/errors/stats/{provider}
Response: {
  provider: "together",
  error_count_24h: 345,
  error_rate_percent: 2.1,
  errors_by_type: {...},
  recent_errors_sample: [...]
}
```

### Financial Endpoints

```bash
GET /api/financial/daily-summary
Response: {
  date: "2025-12-28",
  revenue: 12456.78,
  cost: 3245.67,
  margin_percent: 73.96,
  requests_total: 3500000,
  tokens_total: 12345678
}

GET /api/financial/cost-breakdown
Query params: ?period=24h&group_by=model
Response: {
  period: "24h",
  total_cost: 3245.67,
  by_model: [
    {
      model: "gpt-4o",
      cost: 2431.50,
      percent_of_total: 74.9,
      requests: 2345000,
      tokens: 8934567
    },
    // ... other models
  ],
  by_provider: [...]  // Same structure
}

GET /api/financial/budget
Response: {
  monthly_budget: 100000,
  spent_so_far: 75234.56,
  percent_used: 75.23,
  days_remaining: 3,
  daily_average_spend: 8359.40,
  projected_total: 102831.99,
  over_budget: true,
  alert_level: "critical"
}
```

### SLO & Availability Endpoints

```bash
GET /api/slo/status
Response: {
  availability_target_percent: 99.9,
  current_availability_percent: 99.87,
  compliant: false,  // Very close but below target
  latency_p95_target_ms: 500,
  current_latency_p95_ms: 523,
  latency_compliant: false,
  error_rate_target_percent: 1.0,
  current_error_rate_percent: 0.92,
  error_compliant: true,
  overall_compliant: false,
  period: "current_month"
}

GET /api/providers/{provider}/circuit-breaker
Response: {
  provider: "together",
  breaker_state: "CLOSED",  // or "OPEN", "HALF_OPEN"
  failure_count: 2,
  failure_threshold: 10,
  last_failure: "2025-12-28T09:45:00Z",
  time_in_state_seconds: 3600,
  success_rate_since_last_failure: 99.2
}

GET /api/availability/timeline
Query params: ?period=24h
Response: {
  interval_seconds: 3600,  // 1-hour intervals
  providers: [
    {
      slug: "openrouter",
      timeline: [
        { timestamp: "2025-12-27T10:00:00Z", uptime_percent: 100 },
        { timestamp: "2025-12-27T11:00:00Z", uptime_percent: 99.8 },
        // ... 24 more hours
      ]
    },
    // ... 16 more providers
  ]
}
```

### Token Endpoints

```bash
GET /api/tokens/usage
Query params: ?period=24h
Response: {
  period: "24h",
  total_input_tokens: 234567890,
  total_output_tokens: 123456789,
  total_tokens: 358024679,
  tokens_per_second: 4141,
  cost_per_million_tokens: 2.85
}

GET /api/tokens/by-model
Query params: ?period=7d&limit=10
Response: [
  {
    model: "gpt-4o",
    input_tokens: 456789012,
    output_tokens: 234567890,
    total_tokens: 691356902,
    avg_input_per_request: 5234,
    avg_output_per_request: 2700,
    cost_per_token: 0.00000344
  },
  // ... 9 more models
]

GET /api/tokens/efficiency
Response: {
  estimated_tokens: 340000000,
  actual_tokens: 358024679,
  efficiency_ratio: 94.7,  // 0-100, where 100 = perfect estimate
  waste_percent: 5.3,
  opportunities: [
    "Model A is running 8% over estimate",
    "Model B is 3% under estimate (good!)"
  ]
}
```

---

## ğŸ“‹ Implementation Questions for Data Team

### 1. **Data Source Architecture**
- [ ] Are metrics coming from a metrics database (InfluxDB, Prometheus)?
- [ ] Are they coming from application logs + analytics backend?
- [ ] Are they pre-aggregated or computed on-the-fly?
- [ ] What's the latency for real-time metrics (5s, 30s, 1m)?

### 2. **Granularity & Storage**
- [ ] How far back is 1-minute granular data available? (24h, 7d?)
- [ ] How far back is 1-hour granular data available? (30d, 90d?)
- [ ] Do you retain raw logs beyond a certain period?
- [ ] Can you do custom aggregations on-demand or are they pre-computed?

### 3. **Financial Data**
- [ ] How is "daily revenue" calculated? (from billing system, usage * price?)
- [ ] Where does cost data come from? (provider invoices, pre-calculated, real-time?)
- [ ] Are costs finalized daily or updated continuously?
- [ ] Do you have monthly budget allocations per model?
- [ ] How are "cost optimization tips" generated? (rule-based, ML-based?)

### 4. **Model & Provider Definitions**
- [ ] Do you have a canonical list of supported models with metadata?
- [ ] Does this include context window, max tokens, cost per token?
- [ ] Do you have assigned colors per provider for consistent visualization?
- [ ] How often do new models/providers get added?

### 5. **Error & Incident Data**
- [ ] Where are errors logged? (Loki, ELK, application database?)
- [ ] How long is error history retained?
- [ ] Can you query errors by provider, model, error type, time range?
- [ ] Do you track error stack traces and request payloads (sanitized)?

### 6. **Circuit Breaker & Health**
- [ ] How is circuit breaker state tracked? (Redis, database?)
- [ ] How is health score calculated? (uptime %, error %, latency %)
- [ ] What are the thresholds for healthy/warning/critical?
- [ ] Is this pre-calculated or computed on request?

### 7. **Data Consistency & Freshness**
- [ ] What's the clock skew tolerance across systems?
- [ ] Are metrics eventually consistent or strongly consistent?
- [ ] What happens if a provider is unreachable (how is unavailability tracked)?
- [ ] How do you handle metric delays in real-time dashboards?

---

## ğŸ¯ Next Steps

Once you provide answers to these questions and/or endpoint URLs, I will:

1. **Validate endpoint structures** - Ensure they can support all dashboard visualizations
2. **Design datasource configs** - Create JSON API datasource configs for Grafana
3. **Build all 6 dashboards** - 48 panels with proper queries, transformations, thresholds
4. **Implement drill-down navigation** - Dashboard links with variable passing
5. **Set color schemes** - Consistent provider colors across all dashboards
6. **Configure variables** - Time range, provider filter, model filter, etc.
7. **Set refresh rates** - 5s-300s based on data volatility
8. **Create documentation** - Dashboard guide explaining each panel and how to use

---

**Ready for**: Endpoint specifications, data samples, or architectural clarification
**Current Branch**: `feature/comprehensive-analytics-dashboards`
**Timeline**: Implementation ready upon data requirements confirmation
