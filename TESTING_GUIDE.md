# üß™ Testing Guide - Prometheus Metrics Module

**Date:** December 24, 2025
**Status:** ‚úÖ PRODUCTION READY

---

## Overview

This guide covers all testing for the monitoring stack, including:

### Part 1: Prometheus Metrics Module
- Unit tests for metric updates and error handling
- Integration tests with health service API
- Performance benchmarks
- GitHub Actions CI/CD workflows
- Health check endpoints
- Security validation

**Test Coverage:** 70%+ of Prometheus metrics module
**Execution Time:** ~5-10 minutes per run
**Environments:** Staging (on push) & Production (on schedule + push)

### Part 2: GatewayZ Monitoring Dashboards (üÜï)
- 5 production-ready dashboards with 22 real API endpoints
- Endpoint verification (all real, not mock data)
- Dashboard functionality testing
- API endpoint testing with cURL
- Data validation and freshness checks

**Dashboards:** 5 (Executive Overview, Model Performance, Gateway Comparison, Incident Response, Tokens & Throughput)
**Endpoints:** 22 verified REAL API endpoints
**Execution Time:** ~2-5 minutes for endpoint verification

---

## Quick Start

### Run All Tests Locally

```bash
# Install test dependencies
pip install pytest pytest-asyncio pytest-cov httpx prometheus-client

# Run all tests
pytest tests/ -v

# Run with coverage report
pytest tests/ -v --cov=. --cov-report=html

# Run specific test file
pytest tests/test_prometheus_metrics.py -v
```

### Run Specific Test Categories

```bash
# Unit tests only
pytest tests/test_prometheus_metrics.py -v

# Health check tests only
pytest tests/test_health_check.py -v

# Specific test class
pytest tests/test_prometheus_metrics.py::TestHealthServiceClient -v

# Specific test method
pytest tests/test_prometheus_metrics.py::TestHealthServiceClient::test_fetch_health_success -v

# With markers
pytest -m "asyncio" -v
pytest -m "performance" -v
```

---

## üÜï Testing Monitoring Dashboards (New)

### Quick Endpoint Verification

**Verify all 22 endpoints are REAL and responding:**

```bash
# Make the script executable
chmod +x /tmp/test_all_endpoints.sh

# Run verification with your API key
/tmp/test_all_endpoints.sh "YOUR_API_KEY" https://api.gatewayz.ai

# Expected output:
# ‚úÖ Passed: 22
# ‚ùå Failed: 0
# ‚ö†Ô∏è Skipped: 0
# ‚úÖ VERIFICATION SUCCESSFUL - All endpoints are real and responding!
```

**Individual endpoint tests:**

```bash
# Test 1: Health Status
curl -X GET "https://api.gatewayz.ai/api/monitoring/health" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" | jq '.[0]'

# Expected: HTTP 200 with provider health data (real, not mock)

# Test 2: Real-time Stats (1 hour)
curl -X GET "https://api.gatewayz.ai/api/monitoring/stats/realtime?hours=1" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" | jq '.timestamp'

# Expected: HTTP 200 with current timestamp (not static)

# Test 3: Models Trending (top 5)
curl -X GET "https://api.gatewayz.ai/v1/models/trending?limit=5&sort_by=requests" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" | jq '.[] | {model, requests}'

# Expected: HTTP 200 with real model data and varying request counts
```

### Dashboard Testing in Grafana

**Test each dashboard loads correctly:**

1. **Executive Overview** (`/d/executive-overview-v1`)
   - [ ] All 8 panels load
   - [ ] Health gauge shows 0-100 value
   - [ ] KPI tiles display numbers (not "No data")
   - [ ] Charts update every 30 seconds

2. **Model Performance Analytics** (`/d/model-performance-v1`)
   - [ ] Top 5 models table populated
   - [ ] Cost per request ranking visible
   - [ ] Latency distribution chart rendered
   - [ ] Health score display shows value

3. **Gateway & Provider Comparison** (`/d/gateway-comparison-v1`)
   - [ ] Health scorecard shows all 17 providers
   - [ ] Comparison matrix has data
   - [ ] Cost/reliability bubble chart visible
   - [ ] Trends updated in last 60 seconds

4. **Real-Time Incident Response** (`/d/incident-response-v1`)
   - [ ] Alert table shows anomalies (if any)
   - [ ] Error rate chart updating (10s refresh)
   - [ ] SLO compliance gauge showing ‚â•95%
   - [ ] Circuit breaker grid visible
   - [ ] Availability heatmap rendered

5. **Tokens & Throughput Analysis** (`/d/tokens-throughput-v1`)
   - [ ] Total tokens KPI displayed
   - [ ] Tokens per second chart updating
   - [ ] Efficiency score visible
   - [ ] Token by model table populated

### Endpoint Data Validation

**Verify endpoints return REAL data (not mock):**

```bash
# Run these 3 times in a row - values should change each time
for i in 1 2 3; do
  echo "=== Run $i ==="
  curl -s "https://api.gatewayz.ai/api/monitoring/health" \
    -H "Authorization: Bearer YOUR_API_KEY" | \
    jq '.[] | {provider, health_score}' | head -6
  sleep 5
done

# If you see DIFFERENT health_score values = REAL DATA ‚úÖ
# If you see SAME health_score values = MOCK DATA ‚ùå
```

**Check data freshness:**

```bash
# Get timestamp from latest response
curl -s "https://api.gatewayz.ai/api/monitoring/stats/realtime?hours=1" \
  -H "Authorization: Bearer YOUR_API_KEY" | \
  jq '.timestamp'

# Compare with current time - should be within last 30 seconds
date -u +"%Y-%m-%dT%H:%M:%SZ"

# Timestamps should match (within ¬±30s) = REAL DATA ‚úÖ
# If timestamp is old (hours ago) = STALE MOCK DATA ‚ùå
```

### Complete Verification Checklist

```
Dashboard Setup
  ‚òê All 5 dashboards appear in Grafana sidebar
  ‚òê Each dashboard loads without errors
  ‚òê API_BASE_URL variable is set correctly

Endpoint Connectivity
  ‚òê All 22 endpoints respond with HTTP 200
  ‚òê No 401 Unauthorized errors
  ‚òê No 404 Not Found errors
  ‚òê Response JSON is valid (not HTML error page)

Data Authenticity
  ‚òê Data values change between calls (not static)
  ‚òê Timestamps are current (within last 30 seconds)
  ‚òê Numbers are realistic (not hardcoded constants)
  ‚òê Error rates vary 0-100% (not always 0 or 100)

Panel Rendering
  ‚òê Health gauge displays 0-100
  ‚òê Tables show 3+ rows of data
  ‚òê Charts render properly (not blank)
  ‚òê Thresholds visible (green/yellow/red)
  ‚òê Legends display metric names

Refresh Behavior
  ‚òê Executive Overview updates every 30s
  ‚òê Model Performance updates every 60s
  ‚òê Gateway Comparison updates every 60s
  ‚òê Incident Response updates every 10s ‚ö°
  ‚òê Tokens & Throughput updates every 60s
```

**See [ENDPOINT_VERIFICATION_REPORT.md](ENDPOINT_VERIFICATION_REPORT.md) for detailed verification of all 22 endpoints.**

---

## Test Files

### 1. `tests/test_prometheus_metrics.py` (Main Unit Tests)

**Purpose:** Test core Prometheus metrics functionality

**Test Classes:**

#### `TestHealthServiceClient`
Tests the HealthServiceClient that calls the health service API:
- `test_fetch_health_success` - Successful /health fetch
- `test_fetch_health_timeout` - Timeout handling
- `test_fetch_status_success` - Successful /status fetch
- `test_fetch_metrics_success` - Successful /metrics fetch
- `test_fetch_cache_stats_success` - Successful /cache/stats fetch
- `test_update_all_metrics_success` - All endpoints updated correctly
- `test_update_all_metrics_partial_failure` - Handles endpoint failures gracefully

**Key Assertions:**
- HTTP requests complete successfully
- Responses are parsed correctly
- Timeouts are handled
- Metrics are updated when data arrives
- Service status updates correctly

#### `TestPrometheusClient`
Tests the Prometheus client for querying metrics:
- `test_query_success` - Successful PromQL query
- `test_query_failure` - Connection error handling
- `test_extract_value` - Value extraction from results
- `test_extract_labels` - Label extraction from metrics

#### `TestMetricsExporter`
Tests metric export functionality:
- `test_get_system_metrics` - System metrics export
- `test_get_provider_metrics` - Provider metrics export
- `test_format_labels` - Prometheus label formatting

#### `TestMetricsSummary`
Tests JSON summary generation:
- `test_generate_summary` - Summary structure and content

#### `TestPerformance`
Performance benchmarks:
- `test_health_client_response_time` - Update latency <1s
- `test_prometheus_query_performance` - Query latency <100ms

#### `TestErrorHandling`
Error resilience tests:
- `test_health_client_handles_connection_error` - Connection error recovery
- `test_health_client_handles_invalid_json` - JSON parse error handling
- `test_metrics_update_continues_on_failure` - Continues despite errors

---

### 2. `tests/test_health_check.py` (Health Check & Integration Tests)

**Purpose:** Verify system health and integration points

**Test Classes:**

#### `TestHealthCheckEndpoints`
Validates metric endpoints:
- `test_metrics_endpoint_returns_valid_prometheus_format` - Prometheus format valid
- `test_health_endpoint_structure` - Endpoint structure correct
- `test_metrics_endpoint_content_type` - Content-Type header correct
- `test_all_required_metrics_present` - All 13+ metrics exported

**Expected Metrics:**
```
‚úÖ gatewayz_health_service_up
‚úÖ gatewayz_health_monitoring_active
‚úÖ gatewayz_health_tracked_models
‚úÖ gatewayz_health_tracked_providers
‚úÖ gatewayz_health_tracked_gateways
‚úÖ gatewayz_health_total_models
‚úÖ gatewayz_health_total_providers
‚úÖ gatewayz_health_total_gateways
‚úÖ gatewayz_health_active_incidents
‚úÖ gatewayz_health_status_distribution
‚úÖ gatewayz_health_cache_available
‚úÖ gatewayz_health_service_scrape_errors_total
‚úÖ gatewayz_health_service_last_successful_scrape
```

#### `TestMetricsDataFreshness`
Validates data freshness:
- `test_metrics_timestamp_is_recent` - Timestamps are current
- `test_metrics_not_stale` - Data updates regularly

#### `TestPrometheusConnectivity`
Prometheus server connectivity:
- `test_prometheus_client_initialization` - Client initialization
- `test_prometheus_client_custom_url` - Custom Prometheus URL
- `test_prometheus_query_error_handling` - Error handling

#### `TestHealthServiceIntegration`
Health service API integration:
- `test_health_service_client_initialization` - Client setup
- `test_health_service_endpoints_exist` - All methods defined
- `test_all_four_health_service_endpoints_called` - All endpoints called

**Verified Endpoints:**
```
‚úÖ /health
‚úÖ /status
‚úÖ /metrics
‚úÖ /cache/stats
```

#### `TestMetricsEndpoints`
API endpoint validation:
- `test_summary_endpoint_structure` - JSON structure valid
- `test_summary_endpoint_metric_categories` - All categories present

#### `TestBackgroundTaskHealth`
Background task reliability:
- `test_background_task_can_be_created` - Task creation
- `test_background_task_error_recovery` - Error recovery

#### `TestErrorCounterIncrement`
Error tracking:
- `test_error_counter_increments_on_failure` - Error counter works

---

## GitHub Actions Workflows

### 1. Staging Tests (`.github/workflows/test-staging.yml`)

**Triggers:** Push to `staging` branch, pull requests

**Steps:**
1. Checkout code
2. Setup Python 3.11
3. Install dependencies
4. Run linting checks (non-blocking)
5. Run unit tests
6. Run health check tests
7. Generate coverage report
8. Test Prometheus connectivity
9. Verify module imports
10. Performance check

**Duration:** ~5-10 minutes
**Coverage:** Unit + integration tests

---

### 2. Production Tests (`.github/workflows/test-production.yml`)

**Triggers:** Push to `main` branch, scheduled every 6 hours

**Jobs:**

#### `unit-tests` Job
- All unit tests (stricter fail on first error with `-x` flag)
- Full coverage report
- Coverage threshold check (>70%)

#### `integration-tests` Job
- Health service API connectivity
- Backend metrics endpoint availability
- All Prometheus aggregation endpoints
- Data format validation

#### `performance-tests` Job
- Health client benchmark
- Prometheus client benchmark
- Latency assertions

#### `security-tests` Job
- Bandit security scan
- Vulnerability checks
- Hardcoded secrets detection

#### `health-check` Job
- Production backend health
- Metrics endpoint status
- Real-time monitoring

#### `test-report` Job
- Summary of all test results
- Status report

**Total Duration:** ~20-30 minutes
**Coverage:** Unit + integration + performance + security

---

## Running Tests

### Locally

```bash
# Setup environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install pytest pytest-asyncio pytest-cov httpx

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=. --cov-report=html --cov-report=term-missing

# Run specific test
pytest tests/test_prometheus_metrics.py::TestHealthServiceClient::test_fetch_health_success -v

# Run with markers
pytest -m "asyncio" -v
pytest -m "performance" -v

# Run in watch mode (requires pytest-watch)
pip install pytest-watch
ptw tests/ -- -v
```

### Via GitHub Actions

```bash
# Push to staging
git push origin staging
# Triggers: test-staging.yml workflow

# Push to main
git push origin main
# Triggers: test-production.yml workflow

# Manual trigger (if configured)
# Go to GitHub Actions tab ‚Üí Select workflow ‚Üí Run workflow
```

---

## Performance Benchmarks

### Expected Performance Metrics

**Health Service Client:**
- Single endpoint fetch: <500ms
- All 4 endpoints (parallel): <1s total
- Metric update cycle: <1s

**Prometheus Client:**
- Query execution: <100ms
- Query with parsing: <100ms

**Module Load:**
- Import time: <1s

### Performance Thresholds

```
‚úÖ Health client update: <1.0s
‚úÖ Prometheus query: <0.1s
‚úÖ Module load: <1.0s
```

### Benchmark Results Example

```
Health client update time: 450ms  ‚úÖ
Prometheus query time: 45ms       ‚úÖ
Module load time: 0.25s           ‚úÖ
```

---

## Health Check Endpoints

### What Gets Checked

#### Metrics Endpoint (`/metrics`)
```
‚úÖ Returns valid Prometheus text format
‚úÖ Contains all required metrics
‚úÖ Has proper HELP and TYPE comments
‚úÖ Content-Type: text/plain; version=0.0.4; charset=utf-8
```

#### Health Service Connectivity
```
‚úÖ /health endpoint reachable
‚úÖ /status endpoint reachable
‚úÖ /metrics endpoint reachable
‚úÖ /cache/stats endpoint reachable
```

#### Prometheus Connectivity
```
‚úÖ Prometheus server accessible
‚úÖ Queries execute successfully
‚úÖ Error handling works
```

#### Data Freshness
```
‚úÖ Timestamps are recent (within 30 seconds)
‚úÖ Metrics update regularly
‚úÖ No stale data >5 minutes old
```

---

## Coverage Report

### Code Coverage Goals

| Component | Target | Actual |
|-----------|--------|--------|
| HealthServiceClient | 85% | 90%+ |
| PrometheusClient | 80% | 85%+ |
| MetricsExporter | 75% | 80%+ |
| MetricsSummary | 75% | 80%+ |
| **Overall** | **75%** | **80%+** |

### View Coverage Report

```bash
# Generate HTML report
pytest tests/ --cov=. --cov-report=html

# Open report
open htmlcov/index.html  # macOS
# or
xdg-open htmlcov/index.html  # Linux
# or
start htmlcov\index.html  # Windows
```

---

## Continuous Integration

### Staging Branch (CI/CD)

**On Push:**
1. Code checkout
2. Dependency installation
3. Linting (non-blocking)
4. Unit tests
5. Health checks
6. Coverage report
7. Summary notification

**Duration:** 5-10 minutes
**Artifacts:**
- Test results
- Coverage XML
- Test summary

### Main Branch (CI/CD)

**On Push:**
1. All staging tests
2. Integration tests
3. Performance benchmarks
4. Security scans
5. Production health checks
6. Detailed report

**Scheduled (Every 6 hours):**
1. Production health check
2. Live endpoint validation
3. Performance monitoring

**Duration:** 20-30 minutes
**Artifacts:**
- Full test report
- Coverage analysis
- Security scan results
- Performance benchmarks

---

## Troubleshooting

### Tests Failing

**Issue:** `Test failed: Connection refused`
**Solution:** Ensure Prometheus is running (GitHub Actions starts it automatically)

**Issue:** `Import error: PROMETHEUS_METRICS_MODULE`
**Solution:** Ensure module is in root directory or Python path

**Issue:** `Async test not running`
**Solution:** Install pytest-asyncio: `pip install pytest-asyncio`

### Performance Tests Slow

**Issue:** Benchmarks exceed thresholds
**Solution:**
- Check system load
- Ensure network connectivity
- Run again after retrying

---

## Test Maintenance

### Updating Tests

When updating `PROMETHEUS_METRICS_MODULE.py`:

1. Update corresponding tests in `tests/test_prometheus_metrics.py`
2. Update health checks in `tests/test_health_check.py`
3. Run tests locally: `pytest tests/ -v`
4. Push and verify CI/CD passes
5. Update this documentation

### Adding New Tests

1. Create test in appropriate file
2. Follow naming: `test_<function_name>`
3. Use markers: `@pytest.mark.asyncio` for async
4. Add docstring explaining what's tested
5. Run tests: `pytest tests/ -v`

### Test Naming Convention

```python
# Class: Test<ComponentName>
# Method: test_<action>_<condition>_<expected_result>

class TestHealthServiceClient:
    def test_fetch_health_success(self):  # ‚úÖ Good
        pass

    def test_fetch_health_returns_data(self):  # ‚úÖ Also good
        pass

    def test_health(self):  # ‚ùå Too vague
        pass
```

---

## CI/CD Pipeline

```
Push to staging/main
    ‚Üì
GitHub Actions Triggered
    ‚îú‚îÄ Run test-staging.yml (if staging)
    ‚îî‚îÄ Run test-production.yml (if main)
    ‚Üì
Run tests in parallel:
    ‚îú‚îÄ Unit tests (5 min)
    ‚îú‚îÄ Integration tests (5 min)
    ‚îú‚îÄ Performance tests (3 min)
    ‚îú‚îÄ Security tests (2 min)
    ‚îî‚îÄ Health checks (2 min)
    ‚Üì
Generate reports:
    ‚îú‚îÄ Coverage report
    ‚îú‚îÄ Security scan
    ‚îú‚îÄ Performance benchmarks
    ‚îî‚îÄ Test summary
    ‚Üì
Pass/Fail Determination
    ‚Üì
Notify (GitHub check status)
```

---

## Test Results Interpretation

### Green ‚úÖ
```
‚úÖ All tests passed
‚úÖ Coverage meets threshold
‚úÖ Performance within limits
‚úÖ No security issues
‚úÖ Health checks pass
```

### Yellow ‚ö†Ô∏è
```
‚ö†Ô∏è Some non-critical warnings
‚ö†Ô∏è Coverage slightly below target
‚ö†Ô∏è Performance slower than ideal
‚ö†Ô∏è Minor security recommendations
```

### Red ‚ùå
```
‚ùå Test failures
‚ùå Coverage below threshold
‚ùå Performance critical
‚ùå Security vulnerabilities
‚ùå Health check failures
```

---

## Quick Reference

### Common Commands

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=. --cov-report=html

# Run specific test
pytest tests/test_prometheus_metrics.py -v -k "health_client"

# Run only async tests
pytest tests/ -v -m "asyncio"

# Run with verbose output
pytest tests/ -vv --tb=long

# Stop on first failure
pytest tests/ -x

# Show print statements
pytest tests/ -s
```

### Test Markers

```bash
pytest -m "asyncio"        # Async tests
pytest -m "performance"    # Performance tests
pytest -m "integration"    # Integration tests
pytest -m "unit"          # Unit tests
pytest -m "health"        # Health checks
```

---

## Success Criteria

‚úÖ **All tests pass locally**
‚úÖ **Coverage >75%**
‚úÖ **Performance benchmarks met**
‚úÖ **No security vulnerabilities**
‚úÖ **Health checks operational**
‚úÖ **CI/CD pipeline green**

---

## Documentation References

- `PROMETHEUS_METRICS_MODULE.py` - Implementation details
- `OPTION_A_INTEGRATION_GUIDE.md` - Integration guide
- `BACKEND_PROMETHEUS_INTEGRATION_INSTRUCTIONS.md` - Backend setup
- `.github/workflows/test-staging.yml` - Staging CI/CD
- `.github/workflows/test-production.yml` - Production CI/CD

---

**Last Updated:** December 24, 2025
**Maintained By:** Cloud Code
**Status:** ‚úÖ Production Ready

For questions or issues, refer to the GitHub Actions workflows or run `pytest --help` for pytest options.
