# GatewayZ Observability Stack

**Production-ready observability platform for GatewayZ AI Backend** - Centralized metrics, logs, traces, and visualization with horizontal scaling via Grafana Mimir.

[![Railway Deploy](https://railway.app/button.svg)](https://railway.app)

---

## ðŸš€ Quick Start

### Production Access
- **Grafana Dashboard:** [https://logs.gatewayz.ai](https://logs.gatewayz.ai)

### Local Development (Docker Compose)

```bash
# 1. Clone and navigate to the stack
git clone <repo-url>
cd railway-grafana-stack

# 2. Configure backend metrics scraping (REQUIRED for data to show)
export FASTAPI_TARGET="host.docker.internal:8000"  # If backend runs on host
# OR
export FASTAPI_TARGET="gatewayz-backend:8000"       # If backend is in Docker network

# 3. Start all services
docker compose up --build

# 4. Access services
open http://localhost:3000  # Grafana (admin/yourpassword123)
open http://localhost:9090  # Prometheus
open http://localhost:9009  # Mimir
```

**ðŸ“– Documentation:**
- **[Complete Documentation Index](docs/docs-index.md)** - Start here for all guides and references
- Quick Links:
  - **[Cheatsheet](docs/cheatsheet.md)** - Common commands and queries
  - **[Troubleshooting](docs/troubleshooting/REMOTE_WRITE_DEBUG.md)** - Fix common issues
  - **[Architecture](docs/architecture/MIMIR.md)** - System design and components

---

## ðŸ“Š Architecture Overview

### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GatewayZ Backend API                                 â”‚
â”‚                      (FastAPI: api.gatewayz.ai)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
      Metrics (Pull)        Logs (Push)           Traces (Push)
        /metrics           :3100/loki/push         :4317 (gRPC)
                                                   :4318 (HTTP)
            â”‚                     â”‚                     â”‚
            â–¼                     â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Prometheus   â”‚     â”‚     Loki      â”‚     â”‚    Tempo      â”‚
    â”‚    :9090      â”‚     â”‚    :3100      â”‚     â”‚    :3200      â”‚
    â”‚               â”‚     â”‚               â”‚     â”‚               â”‚
    â”‚ Scrapes every â”‚     â”‚  Log Storage  â”‚     â”‚ Trace Storage â”‚
    â”‚   15-30s      â”‚     â”‚  30d Retain   â”‚     â”‚  48h Retain   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
            â”‚ remote_write        â”‚ (no connection)     â”‚ metrics_generator
            â”‚ /api/v1/push        â”‚                     â”‚ remote_write
            â–¼                     â”‚                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚               â”‚
    â”‚     Mimir     â”‚         (Loki stores      â”‚     Mimir     â”‚
    â”‚    :9009      â”‚          LOGS, not        â”‚  (span metricsâ”‚
    â”‚               â”‚          metrics - this   â”‚   from traces)â”‚
    â”‚  Long-term    â”‚          is BY DESIGN)    â”‚               â”‚
    â”‚  30d Retain   â”‚                           â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    Grafana    â”‚
                          â”‚     :3000     â”‚
                          â”‚               â”‚
                          â”‚ Queries each  â”‚
                          â”‚ source for    â”‚
                          â”‚ its data type â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼             â–¼             â–¼             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Prometheusâ”‚ â”‚  Mimir   â”‚ â”‚   Loki   â”‚ â”‚  Tempo   â”‚
              â”‚(metrics) â”‚ â”‚(metrics) â”‚ â”‚  (logs)  â”‚ â”‚ (traces) â”‚
              â”‚short-termâ”‚ â”‚long-term â”‚ â”‚          â”‚ â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Important: Data Type Separation

| Component | Stores | Writes To | Notes |
|-----------|--------|-----------|-------|
| **Prometheus** | Metrics (time-series) | **Mimir** via remote_write | Short-term storage, scrapes every 15-30s |
| **Mimir** | Metrics (time-series) | Local filesystem | Long-term storage, 30-day retention |
| **Loki** | Logs (text lines) | Local filesystem | **Does NOT write to Mimir** - logs â‰  metrics |
| **Tempo** | Traces (spans) | Local filesystem + **Mimir** (span metrics only) | Traces stored locally, derived metrics to Mimir |

> **Why Loki doesn't write to Mimir:** Loki stores **log lines** (text data), while Mimir stores **metrics** (numeric time-series). These are fundamentally different data types. Grafana queries Loki directly for logs.

### Key Components

| Service | Port | Purpose | Status |
|---------|------|---------|--------|
| **Grafana 11.5.2** | 3000 | Visualization & dashboards | âœ… 5 dashboard folders |
| **Prometheus 3.2.1** | 9090 | Metrics collection + alerting | âœ… 6 scrape jobs |
| **Mimir 2.11.0** | 9009, 9095 | Long-term metrics storage | âœ… 30-day retention |
| **Loki 3.4** | 3100 | Log aggregation | âœ… 30-day retention |
| **Tempo** | 3200, 4317, 4318 | Distributed tracing | âœ… OTLP endpoints |
| **Redis Exporter** | 9121 | Redis metrics | âœ… Integrated |

---

## ðŸŽ¯ Dashboard Folders

All dashboards use **real API endpoints** with live data from Prometheus/Mimir - no mock data.

| Folder | Purpose | Key Metrics | Status |
|--------|---------|-------------|--------|
| **Model Performance** | AI model metrics | Request rates, latency, token usage, error rates | âœ… Ready |
| **Loki** | Log aggregation | Log search, streaming, volume by level/service | âœ… Ready |
| **Prometheus** | Short-term metrics | Scrape targets, query stats, self-monitoring | âœ… Ready |
| **Tempo** | Distributed tracing | Service graph, span metrics, trace search | âœ… Ready |
| **Mimir** | Long-term metrics | Historical queries, retention stats | âœ… Ready |

### Dashboard Features

#### Model Performance
- **AI Provider Metrics**: Request rates by provider/model
- **Latency Analysis**: P50/P95/P99 percentiles
- **Token Usage**: Input/output token tracking
- **Error Rates**: By provider, model, and error type

#### Loki Logs Dashboard
- **Pure log data** from Loki datasource
- **Log Search**: Real-time filtering and search
- **Log Volume**: Count by level, service, severity

#### Tempo Traces Dashboard
- **Pure trace data** from Tempo datasource
- **Service Graph**: Distributed tracing visualization
- **Span Metrics**: Request duration, error rates by service

#### Mimir Long-term Storage
- **Historical Queries**: 30-day retention for trend analysis
- **Consistent Results**: No data loss on Prometheus restarts

---

## âš™ï¸ Configuration

### Prometheus Scrape Jobs

**File:** `prometheus/prometheus.yml`

| Job Name | Target | Interval | Purpose |
|----------|--------|----------|---------|
| `prometheus` | localhost:9090 | 15s | Self-monitoring |
| `gatewayz_production` | `${FASTAPI_TARGET}` | 15s | **Production API metrics** |
| `redis_exporter` | redis-exporter:9121 | 30s | Redis metrics |
| `gatewayz_data_metrics_production` | `${FASTAPI_TARGET}` | 30s | Provider health, circuit breakers |
| `mimir` | mimir:9009 | 30s | Mimir self-monitoring |

#### âš ï¸ **IMPORTANT: `FASTAPI_TARGET` Configuration**

The `${FASTAPI_TARGET}` placeholder **MUST be set** for local development:

```bash
# For backend on host machine (most common)
export FASTAPI_TARGET="host.docker.internal:8000"

# For backend in same Docker network
export FASTAPI_TARGET="gatewayz-backend:8000"

# Verify Prometheus is scraping
curl http://localhost:9090/api/v1/targets
```

**Without this variable, the Backend Services dashboard will show no API data.**

### Grafana Datasources

**Directory:** `grafana/datasources/datasources.yml`

| Datasource | UID | Type | URL | Purpose |
|------------|-----|------|-----|---------|
| **Prometheus** | `grafana_prometheus` | prometheus | `${PROMETHEUS_INTERNAL_URL}` | Short-term metrics |
| **Mimir** | `grafana_mimir` | prometheus | `${MIMIR_INTERNAL_URL}/prometheus` | Long-term metrics |
| **Loki** | `grafana_loki` | loki | `${LOKI_URL}` | Logs |
| **Tempo** | `grafana_tempo` | tempo | `${TEMPO_INTERNAL_URL}` | Traces |

### Data Retention

| Service | Retention | Compaction | Notes |
|---------|-----------|------------|-------|
| **Prometheus** | 15 days | N/A | Short-term, remote writes to Mimir |
| **Mimir** | 30 days | Every 2h | Horizontal scaling, HA-ready |
| **Loki** | 30 days | Every 10m | TSDB index, filesystem storage |
| **Tempo** | 7 days | 5m blocks | Local filesystem |

---

## ðŸ”Œ Backend Integration

### 1. Expose Metrics Endpoint

**Python (FastAPI):**
```python
from prometheus_client import Counter, Histogram, make_asgi_app
from fastapi import FastAPI

app = FastAPI()

# Mount Prometheus metrics endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# Define custom metrics
request_counter = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code', 'env']
)

latency_histogram = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint', 'env']
)

# Use in your routes
@app.get("/v1/models")
async def list_models():
    request_counter.labels(
        method="GET",
        endpoint="/v1/models",
        status_code="200",
        env="production"
    ).inc()
    return {"models": [...]}
```

### 2. Send Logs to Loki

**Python with python-logging-loki:**
```python
import logging
from logging_loki import LokiHandler

loki_handler = LokiHandler(
    url="http://loki:3100/loki/api/v1/push",
    tags={"app": "gatewayz", "env": "production"},
    version="1"
)

logger = logging.getLogger("gatewayz")
logger.addHandler(loki_handler)
logger.setLevel(logging.INFO)

# Use structured logging
logger.info("Request processed", extra={
    "user_id": "123",
    "duration_ms": 45,
    "model": "gpt-4"
})
```

### 3. Send Traces to Tempo

**Python with OpenTelemetry:**
```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure Tempo exporter
otlp_exporter = OTLPSpanExporter(
    endpoint="http://tempo:4318/v1/traces"
)

# Setup tracing
provider = TracerProvider()
provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

# Instrument your code
with tracer.start_as_current_span("model_inference") as span:
    span.set_attribute("model", "gpt-4")
    span.set_attribute("provider", "openai")
    result = await call_model_api()
```

**ðŸ“– Complete Integration Guide:** [docs/backend/BACKEND_METRICS_REQUIREMENTS.md](docs/backend/BACKEND_METRICS_REQUIREMENTS.md)

---

## ðŸ”§ Troubleshooting

### Issue: Backend Services Dashboard Shows "No Data"

**Symptom:** All Redis and API panels show "No data"

**Root Cause:** `FASTAPI_TARGET` environment variable not set for local builds

**Solution:**
```bash
# Set the environment variable
export FASTAPI_TARGET="host.docker.internal:8000"

# Restart Prometheus
docker compose restart prometheus

# Verify targets are UP
open http://localhost:9090/targets
```

### Issue: Loki Dashboard Shows "Bad Request"

**Cause:** Query syntax issues or no logs ingested

**Fix:**
1. Verify Loki is receiving logs:
   ```bash
   curl http://localhost:3100/metrics | grep loki_distributor_lines_received_total
   ```
2. Test log ingestion:
   ```bash
   curl -X POST http://localhost:3100/loki/api/v1/push \
     -H "Content-Type: application/json" \
     -d '{"streams":[{"stream":{"app":"test"},"values":[["'$(date +%s%N)'","test message"]]}]}'
   ```
3. Check query syntax in dashboard (LogQL, not PromQL)

**ðŸ“– More Solutions:** [docs/troubleshooting/](docs/troubleshooting/)

### Issue: Grafana Datasource Connection Failed

**Cause:** Datasource UID mismatch or incorrect URLs

**Fix:**
1. Check datasource health:
   ```bash
   # Prometheus
   curl http://localhost:9090/-/healthy
   
   # Mimir
   curl http://localhost:9009/ready
   
   # Loki
   curl http://localhost:3100/ready
   
   # Tempo
   curl http://localhost:3200/ready
   ```

2. Verify datasource UIDs in Grafana:
   - Prometheus: `grafana_prometheus`
   - Mimir: `grafana_mimir`
   - Loki: `grafana_loki`
   - Tempo: `grafana_tempo`

3. Check environment variables in `docker-compose.yml`

### Issue: Mimir Not Receiving Data from Prometheus

**Symptom:** Prometheus is scraping metrics but Mimir shows no data, or Grafana's Mimir datasource returns empty results.

**Root Cause:** Missing `X-Scope-OrgID` header in Prometheus remote_write or Grafana datasource configuration.

**Why This Happens:** Even with `multitenancy_enabled: false` in Mimir, both writes (from Prometheus) and reads (from Grafana) require the `X-Scope-OrgID` header. When multi-tenancy is disabled, Mimir uses `anonymous` as the default tenant.

**Solution:**

1. **Verify Prometheus remote_write has the header** (`prometheus/prometheus.yml`):
   ```yaml
   remote_write:
     - url: http://mimir:9009/api/v1/push
       headers:
         X-Scope-OrgID: anonymous
   ```

2. **Verify Grafana Mimir datasource has the header** (`grafana/datasources/datasources.yml`):
   ```yaml
   - name: Mimir
     type: prometheus
     url: ${MIMIR_INTERNAL_URL}/prometheus
     jsonData:
       httpHeaderName1: X-Scope-OrgID
     secureJsonData:
       httpHeaderValue1: anonymous
   ```

3. **Restart services after configuration changes:**
   ```bash
   docker compose restart prometheus grafana
   ```

4. **Verify Mimir is receiving data:**
   ```bash
   # Check Mimir ingester status
   curl http://localhost:9009/ingester/ring

   # Check remote write metrics in Prometheus
   curl http://localhost:9090/api/v1/query?query=prometheus_remote_storage_samples_total

   # Test Mimir query directly
   curl -H "X-Scope-OrgID: anonymous" "http://localhost:9009/prometheus/api/v1/query?query=up"
   ```

**ðŸ“– More Solutions:** [docs/troubleshooting/REMOTE_WRITE_DEBUG.md](docs/troubleshooting/REMOTE_WRITE_DEBUG.md)

---

## âœ¨ Key Features

### ðŸš€ Horizontal Scaling with Mimir
- Long-term metrics storage with 30-day retention.
- Horizontally scalable architecture.
- Remote write from Prometheus.

### ðŸ“Š Golden Signals Monitoring
- **Latency**: P50/P95/P99 percentiles + trends.
- **Traffic**: Request volume and rates.
- **Errors**: Error rate gauge + trends.
- **Saturation**: CPU/Memory/Redis utilization.

### ðŸ” Specialized Dashboards
- **Executive Overview**: High-level KPIs and alerts.
- **Backend Services**: Redis cache hit rates, API performance.
- **Loki Logs**: Deep log search without metrics noise.
- **Tempo Traces**: Distributed tracing and service graphs.

### ðŸ›¡ï¸ Production Grade
- **Alerting**: Email notifications for health scores and SLO breaches.
- **Testing**: Comprehensive integration test suite (90+ tests).
- **Security**: No hardcoded credentials; fully environment-variable driven.

---

## ðŸ“Š Mimir Long-term Storage Setup

Mimir provides **30-day metric retention** with horizontal scaling. This is critical for:
- Historical trend analysis
- Consistent query results across page refreshes
- No data loss on Prometheus restarts

### How Prometheus â†’ Mimir Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    remote_write     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    Mimir    â”‚
â”‚   :9090    â”‚   /api/v1/push      â”‚   :9009     â”‚
â”‚            â”‚   X-Scope-OrgID:    â”‚             â”‚
â”‚  (15d)     â”‚   anonymous         â”‚  (30d)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                   â”‚
      â”‚ scrapes                           â”‚ stores
      â–¼                                   â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Backend  â”‚                     â”‚ /data/mimir/ â”‚
 â”‚ /metrics â”‚                     â”‚   blocks/    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚   tsdb/      â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Verifying Mimir is Working

```bash
# 1. Check Mimir is ready
curl http://localhost:9009/ready
# Expected: "ready"

# 2. Check Mimir ingester ring (must show ACTIVE)
curl http://localhost:9009/ingester/ring | jq '.shards[].state'
# Expected: "ACTIVE"

# 3. Check Prometheus remote_write metrics
curl -s http://localhost:9090/api/v1/query?query=prometheus_remote_storage_samples_total | jq '.data.result[].value[1]'
# Expected: increasing number (samples sent)

# 4. Check for remote_write failures
curl -s http://localhost:9090/api/v1/query?query=prometheus_remote_storage_samples_failed_total | jq '.data.result[].value[1]'
# Expected: 0 or very low

# 5. Query Mimir directly
curl -H "X-Scope-OrgID: anonymous" \
  "http://localhost:9009/prometheus/api/v1/query?query=up"
# Expected: JSON with metric data

# 6. Check Mimir logs for write activity
docker compose logs mimir 2>&1 | grep -i "push" | tail -5
```

### Key Configuration Files

| File | Purpose |
|------|---------|
| `prometheus/prometheus.yml` | `remote_write` config with `X-Scope-OrgID: anonymous` header |
| `mimir/mimir.yml` | Local development config (30d retention) |
| `mimir/mimir-railway.yml` | Railway production config |
| `grafana/datasources/datasources.yml` | Mimir datasource with `X-Scope-OrgID` header |

### Mimir Configuration (mimir.yml)

```yaml
# Key settings for 30-day retention
multitenancy_enabled: false  # Uses "anonymous" tenant
limits:
  compactor_blocks_retention_period: 720h  # 30 days
  max_query_lookback: 720h                 # 30 days
  ingestion_rate: 50000                    # samples/sec
  max_global_series_per_user: 500000       # total series
```

---

## ðŸ”” Alerting Setup

GatewayZ uses **Prometheus Alert Rules** with Grafana for visualization. Alerts are defined in `prometheus/alert.rules.yml` with a focus on actionability and reducing alert fatigue.

### Alert Philosophy

1. **Every alert MUST be actionable** - if you can't fix it, don't alert on it
2. **Fewer high-quality alerts** - 14 essential alerts instead of 25+ noisy ones
3. **Warning vs Critical** - warning = investigate soon, critical = wake someone up
4. **No duplicates** - consolidated overlapping alerts into single actionable items

### Alert Groups (14 Total)

| Group | Alerts | Purpose |
|-------|--------|---------|
| **service_health** | 3 | Is the service up and responding? |
| **api_performance** | 3 | Are API responses healthy? |
| **provider_health** | 3 | Are upstream AI providers working? |
| **infrastructure** | 5 | Is the monitoring stack itself healthy? |

### Current Alerts

#### Service Health (Critical)
| Alert | Trigger | Action |
|-------|---------|--------|
| `GatewayZAPIDown` | Prometheus can't scrape API for 2m | Check Railway deployment |
| `HighErrorRate` | >10% error rate for 5m | Check Loki logs, recent deployments |
| `AvailabilitySLOBreach` | <99.5% success rate over 1h | Initiate incident response |

#### API Performance (Warning)
| Alert | Trigger | Action |
|-------|---------|--------|
| `HighAPILatency` | P95 > 3s for 5m | Check slow endpoints, providers |
| `LatencyDegradation` | 50% latency increase vs 1h ago | Check recent changes, resources |
| `TrafficSpike` | 3x traffic increase for 10m | Analyze traffic, check for abuse |

#### Provider Health (Critical/Warning)
| Alert | Trigger | Action |
|-------|---------|--------|
| `ProviderHighErrorRate` | >20% errors per provider for 5m | Check provider status, failover |
| `SlowProviderResponse` | P95 > 5s per provider for 10m | Monitor provider, adjust timeouts |
| `LowModelHealthScore` | <80% success rate for 5m | Review errors across providers |

#### Infrastructure (Critical/Warning)
| Alert | Trigger | Action |
|-------|---------|--------|
| `ScrapeTargetDown` | Any scrape target down for 5m | Check target health, network |
| `MimirRemoteWriteFailures` | Failed samples to Mimir | Check Mimir health, storage |
| `MimirDown` | Mimir unreachable for 2m | Check container, storage volume |
| `TempoNoTraces` | No traces for 15m | Check OTLP endpoint, backend config |
| `LokiNoLogs` | No logs for 15m | Check Loki health, log shipping |

### Removed Alerts (Noise Reduction)

The following alerts were removed to reduce alert fatigue:

| Removed Alert | Reason |
|---------------|--------|
| `LowAPIRequestRate` | Fires during off-hours/weekends, not actionable |
| `CriticalAPILatency` | Duplicate of HighAPILatency (consolidated) |
| `APIErrorRateIncreasing` | Trend detection causes fatigue with absolute threshold |
| Redis alerts (6) | redis_exporter not reliably scraped; re-add when fixed |
| `HighModelInferenceLatency` | Per-model alerting too noisy for 100+ models |
| `SLOLatencyBreach` (500ms) | Too aggressive for AI inference (2-5s typical) |

### Contact Points Configuration

**File:** `grafana/provisioning/alerting/contact_points.yml`

```yaml
contactPoints:
  - name: ops-email
    receivers:
      - type: email
        settings:
          addresses: your-team@company.com
          singleEmail: false

  - name: critical-email
    receivers:
      - type: email
        settings:
          addresses: oncall@company.com
          singleEmail: false

  - name: observatory-pool-critical
    receivers:
      - type: email
        settings:
          addresses: alerts@company.com
```

### Notification Policies

**File:** `grafana/provisioning/alerting/notification_policies.yml`

Routes alerts to appropriate contact points based on labels:

| Matcher | Contact Point | Group Interval |
|---------|---------------|----------------|
| `severity=critical` | critical-email | 5m |
| `category=traffic_spike, severity=critical` | observatory-pool-critical | 5m |
| `category=error_rate_spike` | observatory-pool-critical/warning | 3-5m |
| `component=redis, severity=critical` | critical-email | 2m |
| `component=slo` | critical-email | 5m |

### Alert Rules

**Directory:** `grafana/provisioning/alerting/rules/`

| File | Alerts |
|------|--------|
| `traffic_anomalies.yml` | Traffic spikes, drops |
| `error_rate_anomalies.yml` | Error rate spikes by provider |
| `latency_anomalies.yml` | P99 latency spikes |
| `availability_anomalies.yml` | Provider availability drops |
| `redis_alerts.yml` | Redis cache issues |
| `slo_burn_rate_alerts.yml` | SLO violation alerts |
| `backend_alerts.yml` | Backend service health |
| `model_alerts.yml` | Model-specific issues |

### Setting Up Email Alerts

1. **Configure SMTP in docker-compose.yml:**
   ```yaml
   grafana:
     environment:
       - GF_SMTP_ENABLED=true
       - GF_SMTP_HOST=smtp.gmail.com:587
       - GF_SMTP_USER=your-email@gmail.com
       - GF_SMTP_PASSWORD=your-app-password
       - GF_SMTP_FROM_ADDRESS=grafana@gatewayz.ai
   ```

2. **Update contact_points.yml with your email addresses:**
   ```yaml
   contactPoints:
     - name: ops-email
       receivers:
         - type: email
           settings:
             addresses: your-team@company.com
   ```

3. **Test email delivery:**
   - Go to Grafana â†’ Alerting â†’ Contact points
   - Click "Test" on any contact point
   - Verify email is received

### Viewing Alerts

- **Grafana UI:** Alerting â†’ Alert rules
- **Prometheus UI:** http://localhost:9090/alerts
- **API:**
  ```bash
  # List firing alerts
  curl http://localhost:3000/api/alertmanager/grafana/api/v2/alerts \
    -H "Authorization: Bearer $GRAFANA_API_KEY"
  ```

---

## ðŸ§ª Testing & Quality Assurance

### Test Coverage

- **25+ Real API Endpoints** - Integration tests with performance validation
- **7 Production Dashboards** - Schema validation and configuration checks
- **90+ Test Methods** - Comprehensive coverage across all components
- **GitHub Actions Workflows** - Automated validation on every deployment

### Quick Testing Commands

```bash
# Setup (required once)
cp .env.example .env
# Edit .env and add your API keys

# Run all tests
pytest tests/ -v

# Dashboard validation
./scripts/validate_dashboards.sh strict

# Endpoint testing
export API_KEY="your_api_key"
./scripts/test_all_endpoints.sh "$API_KEY" https://api.gatewayz.ai

# Specific test categories
pytest tests/test_dashboards.py -v -m dashboard
pytest tests/test_api_endpoints.py -v -m endpoint
```

---

## ðŸš€ Deployment

### Railway Deployment

```bash
# 1. Push to Railway
git push railway main

# 2. Or use Railway CLI
railway up

# 3. Configure environment variables in Railway dashboard
FASTAPI_TARGET=api.gatewayz.ai:443
PROMETHEUS_INTERNAL_URL=http://prometheus.railway.internal:9090
MIMIR_INTERNAL_URL=http://mimir.railway.internal:9009
LOKI_URL=http://loki.railway.internal:3100
TEMPO_INTERNAL_URL=http://tempo.railway.internal:3200
```

### Docker Compose (Local)

```bash
# Start all services
docker compose up --build

# Start specific services
docker compose up grafana prometheus mimir

# View logs
docker compose logs -f grafana

# Stop all services
docker compose down

# Clean volumes (WARNING: deletes all data)
docker compose down -v
```

**ðŸ“– Deployment Guide:** [docs/deployment/RAILWAY_DEPLOYMENT_QUICK_START.md](docs/deployment/RAILWAY_DEPLOYMENT_QUICK_START.md)

---

## ðŸ“š Documentation

### Core Documentation

- **[Documentation Index](docs/docs-index.md)** - Start here for all docs
- **[Backend Integration](docs/backend/BACKEND_METRICS_REQUIREMENTS.md)** - Required metrics and instrumentation
- **[Railway Deployment](docs/deployment/RAILWAY_DEPLOYMENT_QUICK_START.md)** - Deploy to Railway
- **[Mimir Integration](MIMIR_INTEGRATION_SUMMARY.md)** - Horizontal scaling guide
- **[Troubleshooting](docs/troubleshooting/)** - Service-specific fix guides



---

## ðŸ› ï¸ Development

### Project Structure

```
railway-grafana-stack/
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ model_performance/  # Model Performance metrics
â”‚   â”‚   â”œâ”€â”€ loki/               # Loki logs (pure logs)
â”‚   â”‚   â”œâ”€â”€ prometheus/         # Prometheus metrics
â”‚   â”‚   â”œâ”€â”€ tempo/              # Tempo traces (pure traces)
â”‚   â”‚   â””â”€â”€ mimir/              # Mimir long-term metrics
â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â””â”€â”€ datasources.yml     # Prometheus, Mimir, Loki, Tempo
â”‚   â””â”€â”€ provisioning/
â”‚       â”œâ”€â”€ dashboards/
â”‚       â”‚   â””â”€â”€ dashboards.yml
â”‚       â””â”€â”€ alerting/
â”‚           â”œâ”€â”€ rules/
â”‚           â”‚   â””â”€â”€ redis_alerts.yml
â”‚           â””â”€â”€ notification_policies.yml
â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh         # Environment-based configuration
â”‚   â”œâ”€â”€ prometheus.yml        # Scrape jobs + remote_write to Mimir
â”‚   â””â”€â”€ alert.rules.yml       # Alert rules
â”œâ”€â”€ mimir/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ mimir.yml             # Mimir configuration (30d retention)
â”œâ”€â”€ loki/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ loki.yml              # Loki configuration
â”œâ”€â”€ tempo/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh         # Environment-based configuration
â”‚   â””â”€â”€ tempo.yml             # Tempo configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pre-build-cleanup.sh  # Railway pre-deploy cleanup
â”‚   â””â”€â”€ ...                   # Other validation scripts
â”œâ”€â”€ tests/                    # Pytest test suite
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ railway.toml              # Railway deployment configuration
â”œâ”€â”€ docker-compose.yml        # Local development
â””â”€â”€ README.md                 # This file
```

### Contributing

1. Create feature branch: `git checkout -b feature/my-feature`
2. Make changes and test locally with `docker compose up`
3. Run tests: `pytest tests/ -v`
4. Create pull request to `main`

---

## ðŸ“ž Support & Resources

### External Documentation

- [Grafana Documentation](https://grafana.com/docs/grafana/latest/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Mimir Documentation](https://grafana.com/docs/mimir/latest/)
- [Loki Documentation](https://grafana.com/docs/loki/latest/)
- [Tempo Documentation](https://grafana.com/docs/tempo/latest/)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [Railway Documentation](https://docs.railway.app/)

### Getting Help

- **Documentation Issues:** Check [docs/troubleshooting/](docs/troubleshooting/)
- **Backend Integration:** See [docs/backend/BACKEND_METRICS_REQUIREMENTS.md](docs/backend/BACKEND_METRICS_REQUIREMENTS.md)
- **Deployment Help:** Review [docs/deployment/RAILWAY_DEPLOYMENT_QUICK_START.md](docs/deployment/RAILWAY_DEPLOYMENT_QUICK_START.md)

---

## ðŸ“„ License

Proprietary - GatewayZ Network

---

**GatewayZ Observability Stack** Â· Enterprise-grade monitoring for AI infrastructure Â· Powered by Prometheus, Mimir, Loki, Tempo, and Grafana
