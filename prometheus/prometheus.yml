global:
  scrape_interval: 15s # Default scrape interval
  external_labels:
    cluster: gatewayz-monitoring
    environment: ${ENVIRONMENT:-production}

# Alerting configuration
# NOTE: Alertmanager is optional. When not configured, alerts are only
# visible in Prometheus UI at /alerts. For email/slack notifications,
# configure Grafana Alerting instead (see grafana/provisioning/alerting/)
#
# To enable Alertmanager, uncomment and set ALERTMANAGER_TARGET:
# alerting:
#   alertmanagers:
#     - static_configs:
#         - targets:
#             - 'ALERTMANAGER_TARGET'
#       timeout: 10s
#       api_version: v2

# Remote write to Mimir for long-term storage and consistent queries
# This ensures metrics are not lost on Prometheus restart and provides
# consistent data across page refreshes
# NOTE: MIMIR_URL is substituted by entrypoint.sh at runtime
# - Railway: Uses mimir.railway.internal:9009
# - Local: Uses mimir:9009 (Docker Compose network)
remote_write:
  - url: MIMIR_URL/api/v1/push
    name: mimir-remote-write
    remote_timeout: 30s
    # REQUIRED: X-Scope-OrgID header for Mimir tenant identification
    # Even with multitenancy_enabled: false, Mimir requires this header
    # "anonymous" is the default tenant used when multi-tenancy is disabled
    headers:
      X-Scope-OrgID: anonymous
    # Retry on rate limiting (HTTP 429) - critical for reliability
    retry_on_http_429: true
    # Send metric metadata (HELP, TYPE) to Mimir
    send_exemplars: true
    queue_config:
      capacity: 10000
      max_shards: 50
      max_samples_per_send: 2000
      batch_send_deadline: 5s
      min_backoff: 30ms
      max_backoff: 5s
      # Retry on recoverable errors
      retry_on_http_429: true
    # Send metric metadata to Mimir for better query experience
    metadata_config:
      send: true
      send_interval: 1m
      max_samples_per_send: 500
    # Write-ahead log for durability during restarts
    # Samples are persisted to disk and replayed on restart
    write_relabel_configs:
      # Forward all metrics (no filtering) - ensures complete data in Mimir
      - source_labels: [__name__]
        regex: '.*'
        action: keep

rule_files:
  - '/etc/prometheus/alert.rules.yml'
  - '/etc/prometheus/recording_rules_baselines.yml'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # MERGED: Combined gatewayz_api and fastapi_backend (were duplicates)
  # Production GatewayZ Backend API
  # NOTE: FASTAPI_TARGET and FASTAPI_SCHEME are substituted by entrypoint.sh
  - job_name: 'gatewayz_production'
    scheme: FASTAPI_SCHEME
    metrics_path: '/metrics'
    static_configs:
      - targets: ['FASTAPI_TARGET']
    scrape_interval: 15s
    scrape_timeout: 10s
    bearer_token_file: '/etc/prometheus/secrets/production_bearer_token'
    metric_relabel_configs:
      - source_labels: []
        target_label: env
        replacement: production

  # Redis Exporter - scrapes Redis metrics from backend project
  # NOTE: REDIS_EXPORTER_TARGET is substituted by entrypoint.sh at runtime
  # Since Redis Exporter is in a different Railway project (backend),
  # we use the public URL, not internal networking
  # Set REDIS_EXPORTER_URL environment variable in Railway to the public URL
  - job_name: 'redis_exporter'
    scheme: https
    static_configs:
      - targets: ['REDIS_EXPORTER_TARGET']
    scrape_interval: 30s
    scrape_timeout: 10s
    metric_relabel_configs:
      - source_labels: []
        target_label: component
        replacement: redis

  # Health Service Metrics Exporter
  - job_name: 'health_service_exporter'
    scheme: http
    static_configs:
      - targets: ['health-service-exporter:8002']
    scrape_interval: 30s
    scrape_timeout: 10s

  # GatewayZ Prometheus Data Metrics (Provider health, circuit breakers, etc.)
  # This scrapes /prometheus/data/metrics for Grafana alerting
  # NOTE: FASTAPI_TARGET and FASTAPI_SCHEME are substituted by entrypoint.sh
  - job_name: 'gatewayz_data_metrics_production'
    scheme: FASTAPI_SCHEME
    metrics_path: '/prometheus/data/metrics'
    static_configs:
      - targets: ['FASTAPI_TARGET']
    scrape_interval: 30s
    scrape_timeout: 15s
    bearer_token_file: '/etc/prometheus/secrets/production_bearer_token'
    metric_relabel_configs:
      - source_labels: []
        target_label: env
        replacement: production
      - source_labels: []
        target_label: source
        replacement: prometheus_data

  # Mimir - Long-term metrics storage (monitors itself)
  # NOTE: MIMIR_TARGET is substituted by entrypoint.sh at runtime
  # - Railway: Uses mimir.railway.internal:9009
  # - Local: Uses mimir:9009 (Docker Compose network)
  - job_name: 'mimir'
    scheme: http
    static_configs:
      - targets: ['MIMIR_TARGET']
    scrape_interval: 30s
    scrape_timeout: 10s
    metric_relabel_configs:
      - source_labels: []
        target_label: component
        replacement: mimir

  # Tempo - Distributed tracing with span metrics
  # Scrapes span metrics generated from traces (model popularity, latency, etc.)
  # NOTE: TEMPO_TARGET is substituted by entrypoint.sh at runtime
  # - Railway: Uses tempo.railway.internal:3200
  # - Local: Uses tempo:3200 (Docker Compose network)
  - job_name: 'tempo'
    scheme: http
    static_configs:
      - targets: ['TEMPO_TARGET']
    scrape_interval: 15s
    scrape_timeout: 10s
    metric_relabel_configs:
      - source_labels: []
        target_label: component
        replacement: tempo
