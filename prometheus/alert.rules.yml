groups:
  - name: performance_alerts
    interval: 30s
    rules:
      # API Request Rate Alerts
      - alert: LowAPIRequestRate
        expr: sum(rate(fastapi_requests_total[5m])) < 10
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Low API request rate detected"
          description: "API request rate is {{ $value }} req/s, below expected threshold of 10 req/s. This may indicate service issues."

      - alert: HighAPIErrorRate
        expr: sum(rate(fastapi_requests_total{status_code=~'4..|5..'}[5m])) / sum(rate(fastapi_requests_total[5m])) > 0.1
        for: 3m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "High API error rate detected"
          description: "API error rate is {{ $value | humanizePercentage }}, exceeding critical threshold of 10%. Immediate investigation required."

      # API Latency Alerts
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, sum(rate(fastapi_requests_duration_seconds_bucket[5m])) by (le)) > 2.0
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High API latency detected"
          description: "API latency (p95) is {{ $value }}s, exceeding threshold of 2.0s. This indicates slow API response times."

      - alert: CriticalAPILatency
        expr: histogram_quantile(0.95, sum(rate(fastapi_requests_duration_seconds_bucket[5m])) by (le)) > 3.0
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Critical API latency detected"
          description: "API latency (p95) is {{ $value }}s, exceeding critical threshold of 3.0s. Immediate investigation required."

      # Model Inference Alerts
      - alert: HighModelInferenceLatency
        expr: histogram_quantile(0.95, sum(rate(model_inference_duration_seconds_bucket[5m])) by (le, model)) > 5.0
        for: 5m
        labels:
          severity: warning
          component: model
        annotations:
          summary: "High model inference latency for {{ $labels.model }}"
          description: "Model {{ $labels.model }} inference latency (p95) is {{ $value }}s, exceeding threshold of 5.0s."

      - alert: ModelInferenceErrorSpike
        expr: sum(rate(model_inference_requests_total{status="error"}[5m])) / sum(rate(model_inference_requests_total[5m])) > 0.15
        for: 3m
        labels:
          severity: critical
          component: model
        annotations:
          summary: "High model inference error rate"
          description: "Model inference error rate is {{ $value | humanizePercentage }}, exceeding critical threshold of 15%."

      # Provider-Specific Alerts
      - alert: SlowProviderResponse
        expr: histogram_quantile(0.95, sum(rate(model_inference_duration_seconds_bucket{provider!=""}[5m])) by (le, provider)) > 4.0
        for: 5m
        labels:
          severity: warning
          component: provider
        annotations:
          summary: "Slow response from provider {{ $labels.provider }}"
          description: "Provider {{ $labels.provider }} response latency (p95) is {{ $value }}s, exceeding threshold of 4.0s."

      - alert: ProviderErrorSpike
        expr: sum(rate(model_inference_requests_total{status="error", provider!=""}[5m])) by (provider) / sum(rate(model_inference_requests_total{provider!=""}[5m])) by (provider) > 0.2
        for: 3m
        labels:
          severity: critical
          component: provider
        annotations:
          summary: "High error rate for provider {{ $labels.provider }}"
          description: "Provider {{ $labels.provider }} error rate is {{ $value | humanizePercentage }}, exceeding critical threshold of 20%."

  - name: performance_trends
    interval: 1m
    rules:
      # Trend Detection - API Latency Degradation
      - alert: APILatencyDegrading
        expr: |
          (
            histogram_quantile(0.95, sum(rate(fastapi_requests_duration_seconds_bucket[15m])) by (le)) /
            histogram_quantile(0.95, sum(rate(fastapi_requests_duration_seconds_bucket[15m] offset 1h)) by (le))
          ) > 1.5
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "API latency degrading over time"
          description: "API latency (p95) has increased by more than 50% compared to 1 hour ago. Current: {{ $value }}x baseline."

      # Trend Detection - API Error Rate Increase
      - alert: APIErrorRateIncreasing
        expr: |
          (
            sum(rate(fastapi_requests_total{status_code=~'4..|5..'}[15m])) / sum(rate(fastapi_requests_total[15m]))
          ) /
          (
            sum(rate(fastapi_requests_total{status_code=~'4..|5..'}[15m] offset 1h)) / sum(rate(fastapi_requests_total[15m] offset 1h))
          ) > 2.0
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "API error rate increasing over time"
          description: "API error rate has increased by more than 100% compared to 1 hour ago. Current: {{ $value }}x baseline."

  - name: model_health_alerts
    interval: 1m
    rules:
      # Model Health Score Alert - Critical when below 20%
      - alert: LowModelHealthScore
        expr: |
          (
            (
              sum(rate(model_inference_requests_total{status="success"}[10m])) 
              / 
              sum(rate(model_inference_requests_total[10m]))
            ) * 100
          ) < 20
        for: 5m
        labels:
          severity: critical
          component: model
          email: "manjeshprasad21@gmail.com"
        annotations:
          summary: "Overall model health score critically low"
          description: "Overall model health score is {{ $value | humanize }}%, below critical threshold of 20%. Success rate indicates major service degradation. Immediate investigation required."
          dashboard: "http://localhost:3000/d/model-performance-v1/model-performance-analytics"
          
      # Individual Provider Health Score
      - alert: LowProviderHealthScore
        expr: |
          (
            (
              sum(rate(model_inference_requests_total{status="success"}[10m])) by (provider)
              / 
              sum(rate(model_inference_requests_total[10m])) by (provider)
            ) * 100
          ) < 20
        for: 5m
        labels:
          severity: critical
          component: provider
          email: "manjeshprasad21@gmail.com"
        annotations:
          summary: "Provider {{ $labels.provider }} health score critically low"
          description: "Provider {{ $labels.provider }} health score is {{ $value | humanize }}%, below critical threshold of 20%. This provider is experiencing major issues."
          dashboard: "http://localhost:3000/d/gateway-comparison-v1/gateway-comparison"

  - name: redis_alerts
    interval: 30s
    rules:
      # Redis Connection Status
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: "Redis is down"
          description: "Redis instance is not responding. Cache functionality is unavailable. Immediate investigation required."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

      # Redis Memory Usage
      - alert: RedisHighMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is {{ $value | humanize }}%, exceeding 80% threshold. Consider clearing cache or increasing memory allocation."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

      - alert: RedisCriticalMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 2m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: "Redis memory usage critical"
          description: "Redis memory usage is {{ $value | humanize }}%, exceeding 90% threshold. Immediate action required to prevent cache eviction."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

      # Redis Cache Hit Rate
      - alert: LowCacheHitRate
        expr: (redis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total)) * 100 < 50
        for: 10m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Redis cache hit rate is {{ $value | humanize }}%, below 50% threshold. This may indicate cache invalidation issues or cold cache."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

      # Redis Connected Clients
      - alert: RedisHighConnectionCount
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "High Redis connection count"
          description: "Redis has {{ $value }} connected clients, exceeding 100 threshold. This may indicate connection leaks."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

      # Redis Slow Operations
      - alert: RedisSlowCommands
        expr: rate(redis_commands_processed_total[5m]) < 10
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis command processing slow"
          description: "Redis is processing {{ $value | humanize }} commands/sec, below normal threshold. This may indicate performance issues."
          dashboard: "http://localhost:3000/d/backend-services-v1/backend-services"

  - name: infrastructure_alerts
    interval: 1m
    rules:
      # Prometheus Target Down
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Prometheus target down: {{ $labels.job }}"
          description: "Prometheus cannot scrape {{ $labels.job }} target. Metrics collection is interrupted."

      # High Request Rate Spike
      - alert: TrafficSpike
        expr: sum(rate(http_requests_total[5m])) > (sum(rate(http_requests_total[5m] offset 1h)) * 2)
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Traffic spike detected"
          description: "Current request rate is more than 2x the rate from 1 hour ago. This may indicate a traffic surge or potential attack."
          dashboard: "http://localhost:3000/d/executive-overview-v1/executive-overview"

      # SLO Breach - Availability
      - alert: SLOAvailabilityBreach
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"2.."}[1h]))
            /
            sum(rate(http_requests_total[1h]))
          ) * 100 < 99.5
        for: 10m
        labels:
          severity: critical
          component: slo
        annotations:
          summary: "SLO Availability breach"
          description: "Availability SLO is at {{ $value | humanize }}%, below 99.5% target. Error budget is being consumed."
          dashboard: "http://localhost:3000/d/executive-overview-v1/executive-overview"

      # SLO Breach - Latency
      - alert: SLOLatencyBreach
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[1h])) by (le)) > 0.5
        for: 10m
        labels:
          severity: warning
          component: slo
        annotations:
          summary: "SLO Latency breach"
          description: "P95 latency is {{ $value }}s, exceeding 500ms target. Response times are degraded."
          dashboard: "http://localhost:3000/d/executive-overview-v1/executive-overview"
