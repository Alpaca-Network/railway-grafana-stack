apiVersion: 1

datasources:
  - name: Tempo
    uid: grafana_tempo  # Fixed UID to match dashboard references
    type: tempo
    access: proxy
    # Tempo URL - internal DNS for same-project access
    # Grafana and Tempo are in the same Railway project (railway-grafana-stack)
    # Port 3200 is Tempo's query API
    url: ${TEMPO_INTERNAL_URL:-http://tempo.railway.internal:3200}
    isDefault: false
    editable: true
    jsonData:
      # Traces→Metrics: link from a trace span to the corresponding metric.
      # IMPORTANT: span metrics (traces_spanmetrics_*) and service-graph metrics
      # (traces_service_graph_*) are generated by Tempo's metrics_generator and
      # remote-written DIRECTLY to Mimir — they are NOT scraped by Prometheus.
      # Therefore this MUST point to grafana_mimir, not grafana_prometheus.
      tracesToMetrics:
        datasourceUid: grafana_mimir
        spanStartTimeShift: -5m
        spanEndTimeShift: 5m
        tags:
          - key: gen_ai.system
            value: provider
          - key: gen_ai.request.model
            value: model
          - key: service.name
            value: service_name
      # Traces→Logs: navigate from a trace to correlated logs in Loki.
      # The backend's LokiLogHandler sends trace_id as a Loki label,
      # so filterByTraceID searches for {trace_id="<trace_id>"}.
      tracesToLogs:
        datasourceUid: grafana_loki
        spanStartTimeShift: -1h
        spanEndTimeShift: 1h
        filterByTraceID: true
        filterBySpanID: true
        tags:
          - key: gen_ai.system
            value: provider
          - key: service.name
            value: app
      # Node graph: visualize service dependencies from trace data
      nodeGraph:
        enabled: true
      # Service graph: auto-generated from Tempo's metrics_generator (service-graphs processor).
      # Metrics are remote-written to Mimir, NOT scraped by Prometheus.
      # Must point to grafana_mimir or the topology map will always be empty.
      serviceMap:
        datasourceUid: grafana_mimir
      # Traces→Profiles: navigate from a slow Tempo span to the Pyroscope flamegraph
      # recorded at that exact moment in time.
      #
      # How it works:
      #   1. You click a slow span in the Tempo trace viewer.
      #   2. Grafana opens Pyroscope filtered to:
      #        • profileTypeId = process_cpu:cpu:nanoseconds:cpu:nanoseconds
      #        • service_name  = gatewayz-backend   (matched via the tag below)
      #        • time range    = span start – span end (with a small buffer)
      #   3. You see a flamegraph of which Python functions were executing during
      #      that specific request window — no manual timestamp correlation needed.
      #
      # The service.name → service_name mapping bridges the OTel attribute name
      # (dot-separated) to the Pyroscope label name (underscore-separated) that
      # pyroscope_config.py uses when pushing profiles.
      tracesToProfiles:
        datasourceUid: grafana_pyroscope
        # CPU flamegraph — shows which functions burned cycles during the span.
        # Switch to memory:alloc_objects:count:space:bytes in the Grafana UI
        # to investigate allocations instead.
        profileTypeId: process_cpu:cpu:nanoseconds:cpu:nanoseconds
        tags:
          - key: service.name
            value: service_name
