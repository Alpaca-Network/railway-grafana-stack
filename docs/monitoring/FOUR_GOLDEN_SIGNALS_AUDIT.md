# Four Golden Signals Dashboard - Data Pipeline Audit

**Date:** January 20, 2026  
**Dashboard:** `latency-analytics-v1.json`  
**Branch:** `fix/fix-golden-signal`  
**Status:** ‚ö†Ô∏è **DATA PIPELINE ISSUE IDENTIFIED**

---

## Executive Summary

### ‚úÖ **GOOD NEWS: Dashboard Configuration is CORRECT**
The Four Golden Signals dashboard is **properly configured** to use Prometheus/Mimir data with PromQL queries. All panels correctly reference the `grafana_prometheus` datasource.

### ‚ö†Ô∏è **BAD NEWS: NO DATA FLOWING FROM BACKEND**
The metrics **exist in Prometheus/Mimir** but have **NO VALUES** because:
1. **Backend is NOT scraping** - `FASTAPI_TARGET` placeholder not replaced
2. **Production backend DOWN** - All `gatewayz_production` targets showing `value: "0"`
3. **Staging backend DOWN** - All `gatewayz_staging` targets showing `value: "0"`

---

## Dashboard Configuration Audit

### ‚úÖ Datasource Configuration: **CORRECT**

All 17 panels use the proper Prometheus datasource:

```json
"datasource": {
  "type": "prometheus",
  "uid": "grafana_prometheus"
}
```

**Grafana Datasource Setup** (`grafana/datasources/datasources.yml`):
```yaml
- name: Prometheus
  type: prometheus
  uid: grafana_prometheus
  url: ${PROMETHEUS_INTERNAL_URL}  # Points to http://prometheus:9090
```

**Prometheus ‚Üí Mimir Integration** (`prometheus/prom.yml`):
```yaml
remote_write:
  - url: http://mimir:9009/api/v1/push
    name: mimir-remote-write
```

### ‚úÖ PromQL Queries: **ALL CORRECT**

#### **SIGNAL 1: LATENCY** (Lines 108-403)

| Panel | Query | Status |
|-------|-------|--------|
| **P50 Latency** | `histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))` | ‚úÖ Valid |
| **P95 Latency** | `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))` | ‚úÖ Valid |
| **P99 Latency** | `histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))` | ‚úÖ Valid |
| **Latency Trend** | All 3 percentiles over time | ‚úÖ Valid |

**Metric Used:** `http_request_duration_seconds_bucket`  
**Source:** Backend's OpenTelemetry instrumentation (ObservabilityMiddleware)

---

#### **SIGNAL 2: TRAFFIC** (Lines 411-704)

| Panel | Query | Status |
|-------|-------|--------|
| **Request Rate** | `sum(rate(fastapi_requests_total[5m]))` | ‚úÖ Valid |
| **Active Requests** | `sum(fastapi_requests_in_progress) or vector(0)` | ‚úÖ Valid |
| **Total Requests (1h)** | `sum(increase(fastapi_requests_total[1h]))` | ‚úÖ Valid |
| **Traffic by Status** | `sum(rate(fastapi_requests_total{status_code=~"2.."}[5m]))` (2xx, 4xx, 5xx) | ‚úÖ Valid |

**Metrics Used:** 
- `fastapi_requests_total` (with `status_code` label)
- `fastapi_requests_in_progress`

**Source:** FastAPI Prometheus Instrumentator

---

#### **SIGNAL 3: ERRORS** (Lines 712-922)

| Panel | Query | Status |
|-------|-------|--------|
| **Error Rate %** | `sum(rate(fastapi_requests_total{status_code=~"5.."}[5m])) / sum(rate(fastapi_requests_total[5m]))` | ‚úÖ Valid |
| **Total 5xx Errors** | `sum(increase(fastapi_requests_total{status_code=~"5.."}[1h]))` | ‚úÖ Valid |
| **Error Rate Trend** | Error rate over time | ‚úÖ Valid |

**Metric Used:** `fastapi_requests_total{status_code=~"5.."}`

---

#### **SIGNAL 4: SATURATION** (Lines 930-1289)

| Panel | Query | Status |
|-------|-------|--------|
| **CPU Utilization** | `100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)` | ‚ö†Ô∏è **NO DATA** |
| **Memory Utilization** | `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100` | ‚ö†Ô∏è **NO DATA** |
| **Redis Memory %** | `(redis_memory_used_bytes / redis_memory_max_bytes) * 100` | ‚ö†Ô∏è **NO DATA** |
| **Redis Connections** | `redis_connected_clients` | ‚ö†Ô∏è **NO DATA** |
| **Resource Trend** | CPU, Memory, Redis over time | ‚ö†Ô∏è **NO DATA** |

**Metrics Used:**
- `node_cpu_seconds_total` - **MISSING** (requires Node Exporter)
- `node_memory_MemAvailable_bytes`, `node_memory_MemTotal_bytes` - **MISSING** (requires Node Exporter)
- `redis_memory_used_bytes`, `redis_memory_max_bytes`, `redis_connected_clients` - **PARTIAL** (Redis Exporter unhealthy)

---

## Data Pipeline Status

### Prometheus Scrape Targets (from `up` metric)

| Job | Instance | Status | Issue |
|-----|----------|--------|-------|
| `prometheus` | `localhost:9090` | ‚úÖ UP (1) | Self-monitoring OK |
| `mimir` | `mimir:9009` | ‚úÖ UP (1) | Long-term storage OK |
| `gatewayz_production` | **`FASTAPI_TARGET`** | ‚ùå DOWN (0) | **Placeholder not replaced** |
| `gatewayz_staging` | `gatewayz-staging.up.railway.app` | ‚ùå DOWN (0) | Backend unreachable |
| `gatewayz_data_metrics_production` | **`FASTAPI_TARGET`** | ‚ùå DOWN (0) | **Placeholder not replaced** |
| `gatewayz_data_metrics_staging` | `gatewayz-staging.up.railway.app` | ‚ùå DOWN (0) | Backend unreachable |
| `redis_exporter` | `redis-exporter:9121` | ‚ùå DOWN (0) | Container unhealthy |
| `health_service_exporter` | `health-service-exporter:8002` | ‚ùå DOWN (0) | Container not running |

### Metric Availability in Prometheus

**Metrics exist** (defined in metric store):
```bash
$ curl http://localhost:9090/api/v1/label/__name__/values
# Returns 20+ metrics including:
- http_request_duration_seconds_bucket ‚úÖ
- fastapi_requests_total ‚úÖ
- fastapi_requests_in_progress ‚úÖ
- node_cpu_seconds_total ‚ùå (no data)
- redis_memory_used_bytes ‚ùå (no data)
```

**Metrics have data** (actual values):
```bash
$ curl 'http://localhost:9090/api/v1/query?query=http_request_duration_seconds_bucket'
# Returns: [] (EMPTY - NO DATA)

$ curl 'http://localhost:9090/api/v1/query?query=fastapi_requests_total'
# Returns: [] (EMPTY - NO DATA)
```

---

## Mimir Duplicate Timestamp Issue

### Error Analysis

**Observed Error:**
```
ts=2026-01-20T01:42:18.248919912Z caller=push.go:178 level=error user=anonymous 
msg="push error" err="rpc error: code = Code(400) desc = failed pushing to ingester: 
user=anonymous: the sample has been rejected because another sample with the same 
timestamp, but a different value, has already been ingested (err-mimir-sample-duplicate-timestamp). 
The affected sample has timestamp 2026-01-20T01:42:13.032Z and is from series 
{__name__=\"thanos_objstore_bucket_operation_duration_seconds_sum\", 
 cluster=\"gatewayz-monitoring\", component=\"mimir\", instance=\"mimir:9009\", 
 job=\"mimir\", operation=\"get\"}"
```

### Root Cause

**Mimir is scraping itself** (`job=mimir`) and experiencing duplicate timestamps:

1. **Prometheus scrapes Mimir** at `mimir:9009` every 30s
2. **Prometheus writes to Mimir** via `remote_write`
3. **Mimir's internal metrics** (Thanos object store) have timing conflicts
4. **Same timestamp, different values** = duplicate rejection

### Impact Assessment

| Severity | Impact | Notes |
|----------|--------|-------|
| ‚ö†Ô∏è **LOW** | Mimir self-monitoring has gaps | Does NOT affect GatewayZ metrics |
| ‚úÖ **NO IMPACT** | Four Golden Signals unaffected | Uses `fastapi_*` and `http_*` metrics |
| üìä **INFORMATIONAL** | Mimir performance metrics incomplete | Only affects Mimir's own observability |

### Affected Metrics

**Only Mimir's internal metrics:**
- `thanos_objstore_bucket_operation_duration_seconds_*`
- `thanos_objstore_bucket_operation_transferred_bytes_bucket`

**Not affected:**
- `http_request_duration_seconds_bucket` ‚úÖ
- `fastapi_requests_total` ‚úÖ
- All Four Golden Signals metrics ‚úÖ

---

## Critical Issues Blocking Data

### üö® **Issue #1: FASTAPI_TARGET Placeholder Not Replaced**

**File:** `prometheus/prom.yml` (Lines 32-43, 89-103)

**Problem:**
```yaml
- job_name: 'gatewayz_production'
  static_configs:
    - targets: ['FASTAPI_TARGET']  # ‚ùå Literal string, not replaced
```

**Expected:**
```yaml
- job_name: 'gatewayz_production'
  static_configs:
    - targets: ['backend:8000']  # OR production backend URL
```

**Impact:** 
- Prometheus cannot scrape backend metrics
- All `http_request_duration_seconds_*` metrics are EMPTY
- All `fastapi_requests_*` metrics are EMPTY

**Fix Required:**
1. Update `docker-compose.yml` to set `FASTAPI_TARGET` environment variable
2. Use `envsubst` to replace placeholders in `prom.yml`
3. OR hardcode production backend URL

---

### üö® **Issue #2: Missing Node Exporter**

**Problem:**
No Node Exporter container running to provide system metrics.

**Missing Metrics:**
- `node_cpu_seconds_total{mode="idle"}`
- `node_memory_MemAvailable_bytes`
- `node_memory_MemTotal_bytes`
- `node_filesystem_*`

**Impact:**
- **SIGNAL 4: SATURATION** panels show NO DATA
- CPU Utilization: ‚ùå
- Memory Utilization: ‚ùå

**Fix Required:**
Add Node Exporter to `docker-compose.yml`:
```yaml
services:
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    networks:
      - gatewayz-monitoring
```

Then add to Prometheus scrape config:
```yaml
- job_name: 'node_exporter'
  static_configs:
    - targets: ['node-exporter:9100']
```

---

### ‚ö†Ô∏è **Issue #3: Redis Exporter Unhealthy**

**Problem:**
```bash
$ docker ps
railway-grafana-stack-redis-exporter-1   Up 2 minutes (unhealthy)
```

**Impact:**
- `redis_memory_used_bytes` - NO DATA
- `redis_memory_max_bytes` - NO DATA
- `redis_connected_clients` - NO DATA

**Fix Required:**
1. Check Redis Exporter logs: `docker logs railway-grafana-stack-redis-exporter-1`
2. Verify Redis connection string in `docker-compose.yml`
3. Restart container: `docker restart railway-grafana-stack-redis-exporter-1`

---

## Recommendations

### ‚úÖ **Immediate Actions (Fix Data Flow)**

1. **Replace FASTAPI_TARGET placeholder**
   ```bash
   # Option 1: Use environment variable substitution
   export FASTAPI_TARGET="backend:8000"  # OR production URL
   envsubst < prometheus/prom.yml.template > prometheus/prom.yml
   docker restart railway-grafana-stack-prometheus-1
   
   # Option 2: Update docker-compose.yml
   services:
     prometheus:
       environment:
         - FASTAPI_TARGET=backend:8000
       command:
         - '--config.file=/etc/prometheus/prom.yml'
   ```

2. **Add Node Exporter** (for SIGNAL 4: SATURATION)
   - Add service to `docker-compose.yml` (see Issue #2 above)
   - Update Prometheus scrape config
   - Restart stack

3. **Fix Redis Exporter**
   - Check logs for connection errors
   - Verify Redis URL/credentials
   - Restart container

### üîÑ **Medium Priority (Improve Reliability)**

4. **Add GatewayZ Backend to Docker Compose**
   ```yaml
   services:
     backend:
       image: ghcr.io/alpaca-network/gatewayz-backend:latest
       container_name: gatewayz-backend
       ports:
         - "8000:8000"
       environment:
         - SUPABASE_URL=${SUPABASE_URL}
         - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
       networks:
         - gatewayz-monitoring
   ```

5. **Resolve Mimir Duplicate Timestamps** (non-critical)
   - Option A: Disable Mimir self-scraping (remove `job_name: mimir`)
   - Option B: Add `scrape_interval: 60s` to Mimir job (reduce frequency)
   - Option C: Ignore (doesn't affect GatewayZ metrics)

### üìä **Long-term (Enhanced Monitoring)**

6. **Add Metric Validation Tests**
   - Automated checks for metric availability
   - Alert if critical metrics go missing
   - Dashboard data quality monitoring

7. **Implement Metric Cardinality Limits**
   - Prevent duplicate timestamp issues
   - Add metric relabeling rules
   - Configure Mimir limits

---

## Verification Checklist

After implementing fixes, verify data flow:

### Backend Metrics
```bash
# 1. Check Prometheus targets
curl 'http://localhost:9090/api/v1/targets' | jq '.data.activeTargets[] | select(.job == "gatewayz_production") | {job, health, lastScrape}'

# 2. Verify metrics have data
curl 'http://localhost:9090/api/v1/query?query=http_request_duration_seconds_bucket' | jq '.data.result | length'
# Should return > 0

# 3. Test P50 latency query
curl 'http://localhost:9090/api/v1/query?query=histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))' | jq '.data.result[0].value[1]'
# Should return a number (e.g., "0.05" for 50ms)
```

### System Metrics
```bash
# 4. Check Node Exporter
curl 'http://localhost:9090/api/v1/query?query=node_cpu_seconds_total' | jq '.data.result | length'
# Should return > 0

# 5. Check Redis Exporter
curl 'http://localhost:9090/api/v1/query?query=redis_connected_clients' | jq '.data.result[0].value[1]'
# Should return number of connections
```

### Grafana Dashboard
```bash
# 6. Access dashboard
open http://localhost:3000/d/latency-analytics-v1

# 7. Verify panels show data:
# - P50/P95/P99 Latency: Should show values in seconds
# - Request Rate: Should show req/s
# - Error Rate: Should show %
# - CPU/Memory: Should show % utilization
```

---

## Conclusion

### ‚úÖ **Dashboard Configuration: PERFECT**
The Four Golden Signals dashboard is **expertly configured** with:
- Correct Prometheus datasource references
- Valid PromQL queries using proper histogram quantiles
- Appropriate time ranges and aggregations
- Proper metric names from FastAPI and OpenTelemetry

### ‚ùå **Data Pipeline: BROKEN**
The issue is **NOT the dashboard**, it's the **data collection infrastructure**:
1. Backend not being scraped (FASTAPI_TARGET placeholder)
2. Node Exporter missing (system metrics)
3. Redis Exporter unhealthy (Redis metrics)

### üéØ **Next Steps**
1. Fix `FASTAPI_TARGET` placeholder ‚Üí **Immediate data population**
2. Add Node Exporter ‚Üí **SIGNAL 4 (SATURATION) works**
3. Fix Redis Exporter ‚Üí **Complete SIGNAL 4 coverage**
4. Optionally fix Mimir duplicates ‚Üí **Cleaner logs**

**Once these 3 infrastructure issues are resolved, the Four Golden Signals dashboard will be FULLY OPERATIONAL with real-time data from Prometheus/Mimir.**

---

**Audit Completed By:** Claude (OpenCode AI)  
**Date:** January 20, 2026, 01:45 UTC  
**Branch:** `fix/fix-golden-signal`
